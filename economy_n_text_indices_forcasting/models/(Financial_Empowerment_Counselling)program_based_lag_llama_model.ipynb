{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a1e23e37d97f42d692be2247ff1a019a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a4ece866f7b4dc8a663605433c4841f",
              "IPY_MODEL_43f5a674fac14344b443c39da0bffd91",
              "IPY_MODEL_a3827188af4d4ab6ba4fad6855efa127"
            ],
            "layout": "IPY_MODEL_35747ec32f184cb88af9bcbdb55b9492"
          }
        },
        "6a4ece866f7b4dc8a663605433c4841f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72f6acab203a4a029905a256946d0dda",
            "placeholder": "​",
            "style": "IPY_MODEL_fb775fd5c76d42a19c3f894a0e35cae7",
            "value": "Best trial: 46. Best value: 1.38244: 100%"
          }
        },
        "43f5a674fac14344b443c39da0bffd91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7afa295c7aa4faaa6b8fe4919ce28b8",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18751f34953b4d8aa1ea151574d3966a",
            "value": 50
          }
        },
        "a3827188af4d4ab6ba4fad6855efa127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8efecb26c9f148ae9b4d35751080dce6",
            "placeholder": "​",
            "style": "IPY_MODEL_93901f072672411d910702ac1828f071",
            "value": " 50/50 [1:26:18&lt;00:00, 148.46s/it]"
          }
        },
        "35747ec32f184cb88af9bcbdb55b9492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72f6acab203a4a029905a256946d0dda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb775fd5c76d42a19c3f894a0e35cae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7afa295c7aa4faaa6b8fe4919ce28b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18751f34953b4d8aa1ea151574d3966a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8efecb26c9f148ae9b4d35751080dce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93901f072672411d910702ac1828f071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2658e67458f54305892976a4abaec7d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_717d4fb0105b467ba2d5439eeb19a864",
              "IPY_MODEL_c05b09efc94149e290b2b4ecadc06877",
              "IPY_MODEL_3de30e323c4a42dc84721ce455e11f5f"
            ],
            "layout": "IPY_MODEL_3d1b4226fde94c3db00cdb9bd99a2e98"
          }
        },
        "717d4fb0105b467ba2d5439eeb19a864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38a4e932c28a4f4fabe6e9898d048b5b",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f21a0285c949858720eb0265e9045c",
            "value": "Epoch 29: "
          }
        },
        "c05b09efc94149e290b2b4ecadc06877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2873a70a70634a729232a74ed4edf967",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8cda3099e9f428581c26e941c74b9c4",
            "value": 1
          }
        },
        "3de30e323c4a42dc84721ce455e11f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dbaefcb252543f6b8cdc255129f56ca",
            "placeholder": "​",
            "style": "IPY_MODEL_02cac27444574ad7b2249d17ff2d0599",
            "value": " 40/? [00:02&lt;00:00, 19.28it/s, v_num=50]"
          }
        },
        "3d1b4226fde94c3db00cdb9bd99a2e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "38a4e932c28a4f4fabe6e9898d048b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f21a0285c949858720eb0265e9045c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2873a70a70634a729232a74ed4edf967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8cda3099e9f428581c26e941c74b9c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3dbaefcb252543f6b8cdc255129f56ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02cac27444574ad7b2249d17ff2d0599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea85154378304b76a3f01218b9d67fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_672e4acfa83d4e5aae0862545121926e",
              "IPY_MODEL_eaad0efda1f6492895889d662b8fbf48",
              "IPY_MODEL_446541ee2dc24daaa5406a7eb4453e83"
            ],
            "layout": "IPY_MODEL_93f4a96f3c2e4aaf8352b67d09c53ed8"
          }
        },
        "672e4acfa83d4e5aae0862545121926e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5c09c5cb7c442e792e4eb5630595978",
            "placeholder": "​",
            "style": "IPY_MODEL_900781e8f0ed4e64b5b91f2fb55ac9ef",
            "value": "Epoch 99: "
          }
        },
        "eaad0efda1f6492895889d662b8fbf48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed2adbee5ad94a0790d8615b7eb4b53a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d01ba1409095411696e7faf95abeffc9",
            "value": 1
          }
        },
        "446541ee2dc24daaa5406a7eb4453e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48570123bfb7483db8c12f5036c7d06a",
            "placeholder": "​",
            "style": "IPY_MODEL_5b50a03e7f5249e992cde7c0818851a4",
            "value": " 40/? [00:02&lt;00:00, 18.82it/s, v_num=53]"
          }
        },
        "93f4a96f3c2e4aaf8352b67d09c53ed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "d5c09c5cb7c442e792e4eb5630595978": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "900781e8f0ed4e64b5b91f2fb55ac9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed2adbee5ad94a0790d8615b7eb4b53a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d01ba1409095411696e7faf95abeffc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48570123bfb7483db8c12f5036c7d06a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b50a03e7f5249e992cde7c0818851a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " #Run this cell FIRST and ONLY ONCE\n",
        "\n",
        "# 1. Downgrade numpy to fix the \"dtype size changed\" error\n",
        "!pip install \"numpy<2.0\" --force-reinstall\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "MIvC_ZrIHiDd",
        "outputId": "8ff4d33a-3cf9-460e-abd6-cbb5906e112c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2.0\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "3ba835ac69094896964ecedb1912de9c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Install GluonTS and other dependencies\n",
        "!pip install gluonts torch pandas matplotlib seaborn --quiet\n",
        "\n",
        "# 3. Clone and install Lag-Llama\n",
        "import os\n",
        "if not os.path.exists(\"lag-llama\"):\n",
        "    !git clone https://github.com/time-series-foundation-models/lag-llama/\n",
        "\n",
        "%cd lag-llama\n",
        "!pip install -r requirements.txt --quiet\n",
        "\n",
        "print(\"\\n✓ Setup complete! Now run Cell 2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv52b5ojHknV",
        "outputId": "34ffcd8e-bd5f-498b-9be6-855ff0cb7bed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.5 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'lag-llama'...\n",
            "remote: Enumerating objects: 508, done.\u001b[K\n",
            "remote: Counting objects: 100% (183/183), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 508 (delta 155), reused 114 (delta 114), pack-reused 325 (from 3)\u001b[K\n",
            "Receiving objects: 100% (508/508), 286.89 KiB | 11.48 MiB/s, done.\n",
            "Resolving deltas: 100% (253/253), done.\n",
            "/content/lag-llama\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.1/778.1 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "xarray 2025.11.0 requires pandas>=2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "✓ Setup complete! Now run Cell 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BHvvJAXHagQ",
        "outputId": "ebe119ad-953d-462b-b7ff-bf8724166cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/404.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import os\n",
        "import warnings\n",
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# CRITICAL FIX: Patch torch.load ONCE at module level\n",
        "# ---------------------------------------------------------------------------\n",
        "import torch.serialization\n",
        "\n",
        "if not hasattr(torch.serialization, '_original_torch_load_backup'):\n",
        "    torch.serialization._original_torch_load_backup = torch.serialization.load\n",
        "\n",
        "def safe_torch_load(*args, **kwargs):\n",
        "    \"\"\"Wrapper that adds weights_only=False by default\"\"\"\n",
        "    if 'weights_only' not in kwargs:\n",
        "        kwargs['weights_only'] = False\n",
        "    return torch.serialization._original_torch_load_backup(*args, **kwargs)\n",
        "\n",
        "# Apply the patch\n",
        "torch.load = safe_torch_load\n",
        "torch.serialization.load = safe_torch_load\n",
        "\n",
        "print(\"✓ torch.load patched successfully\")\n",
        "\n",
        "# GluonTS imports\n",
        "from gluonts.dataset.pandas import PandasDataset\n",
        "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
        "from lag_llama.gluon.estimator import LagLlamaEstimator\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 1. LOAD AND PREPROCESS DATA\n",
        "# ---------------------------------------------------------------------------\n",
        "filename = '/content/macro_index_Financial_Empowerment_Counselling_only.csv'\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    filename = 'macro_index_Financial_Empowerment_Counselling_only.csv'\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    print(f\"❌ Error: {filename} not found. Please upload your CSV file.\")\n",
        "    raise FileNotFoundError(\"Please upload macro_index_Financial_Empowerment_Counselling_only.csv\")\n",
        "\n",
        "df = pd.read_csv(filename)\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.set_index('date', inplace=True)\n",
        "df = df.asfreq('W-SUN')\n",
        "df = df.ffill().bfill()\n",
        "\n",
        "# CRITICAL: Convert all numeric data to float32\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[num_cols] = df[num_cols].astype('float32')\n",
        "\n",
        "print(f\"✓ Data Loaded. Shape: {df.shape}\")\n",
        "print(f\"✓ Date range: {df.index[0]} to {df.index[-1]}\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 2. FEATURE SELECTION\n",
        "# ---------------------------------------------------------------------------\n",
        "target_col = 'Financial Empowerment Counselling_EMA8'\n",
        "\n",
        "use_exogenous = True\n",
        "\n",
        "if use_exogenous:\n",
        "    selected_features = [\n",
        "        'CPIAUCSL', 'FXEURCAD', 'FXUSDCAD', 'DGS10', 'goc_long_benchmark', 'woodgreen_seniors_care_text_information',\n",
        "        'goc_long_benchmark1', 'goc_avg_over10y', 'DTWEXBGS', 'DFF', 'woodgreen_employment_text_information'\n",
        "    ]\n",
        "    selected_features = [f for f in selected_features if f in df.columns]\n",
        "    print(f\"\\n✓ Using {len(selected_features)} exogenous features\")\n",
        "else:\n",
        "    selected_features = []\n",
        "    print(f\"\\n✓ Using UNIVARIATE forecasting (no exogenous features)\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3. DATA SPLIT STRATEGY\n",
        "# ---------------------------------------------------------------------------\n",
        "prediction_length = 12\n",
        "\n",
        "# Use validation split for hyperparameter tuning\n",
        "# Train: up to -24 weeks, Val: -24 to -12, Test: last 12 weeks\n",
        "full_train_end = len(df) - 24  # Reserve last 24 weeks\n",
        "val_start = full_train_end\n",
        "val_end = len(df) - 12\n",
        "test_start = val_end\n",
        "\n",
        "train_df = df.iloc[:full_train_end]\n",
        "val_df = df.iloc[:val_end]  # Includes train + val for proper context\n",
        "test_df = df.iloc[:test_start]\n",
        "\n",
        "print(f\"\\n✓ Train dataset: {len(train_df)} weeks (up to {train_df.index[-1]})\")\n",
        "print(f\"✓ Validation period: weeks {val_start} to {val_end}\")\n",
        "print(f\"✓ Test period: last {prediction_length} weeks\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4. DOWNLOAD LAG-LLAMA CHECKPOINT (ONE TIME)\n",
        "# ---------------------------------------------------------------------------\n",
        "ckpt_path = \"lag-llama.ckpt\"\n",
        "if not os.path.exists(ckpt_path):\n",
        "    print(\"\\nDownloading Lag-Llama checkpoint...\")\n",
        "    !huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir .\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"✓ Using device: {device}\")\n",
        "\n",
        "# Load base checkpoint parameters\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "base_estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 5. OPTUNA OBJECTIVE FUNCTION\n",
        "# ---------------------------------------------------------------------------\n",
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Optuna objective function to minimize validation MAE\n",
        "    \"\"\"\n",
        "\n",
        "    # Suggest hyperparameters\n",
        "    context_length = trial.suggest_categorical('context_length', [16, 32, 64, 96])\n",
        "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
        "    max_epochs = trial.suggest_int('max_epochs', 20, 100, step=10)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "    aug_prob = trial.suggest_float('aug_prob', 0.0, 0.3)\n",
        "\n",
        "    # Optional: tune model architecture (if you want deeper optimization)\n",
        "    # n_layer = trial.suggest_int('n_layer', 4, 12)\n",
        "    # n_head = trial.suggest_int('n_head', 4, 16)\n",
        "\n",
        "    try:\n",
        "        # Create datasets\n",
        "        if selected_features:\n",
        "            train_ds = PandasDataset(train_df, target=target_col,\n",
        "                                    feat_dynamic_real=selected_features, freq=\"W-SUN\")\n",
        "            val_ds = PandasDataset(val_df, target=target_col,\n",
        "                                  feat_dynamic_real=selected_features, freq=\"W-SUN\")\n",
        "        else:\n",
        "            train_ds = PandasDataset(train_df, target=target_col, freq=\"W-SUN\")\n",
        "            val_ds = PandasDataset(val_df, target=target_col, freq=\"W-SUN\")\n",
        "\n",
        "        # Create estimator with trial hyperparameters\n",
        "        estimator = LagLlamaEstimator(\n",
        "            ckpt_path=ckpt_path,\n",
        "            prediction_length=prediction_length,\n",
        "            context_length=context_length,\n",
        "            n_layer=base_estimator_args[\"n_layer\"],  # Or use trial.suggest_int\n",
        "            n_embd_per_head=base_estimator_args[\"n_embd_per_head\"],\n",
        "            n_head=base_estimator_args[\"n_head\"],  # Or use trial.suggest_int\n",
        "            scaling=base_estimator_args[\"scaling\"],\n",
        "            time_feat=base_estimator_args[\"time_feat\"],\n",
        "            aug_prob=aug_prob,\n",
        "            batch_size=batch_size,\n",
        "            num_parallel_samples=100,\n",
        "            lr=learning_rate,\n",
        "            trainer_kwargs={\n",
        "                \"accelerator\": device,\n",
        "                \"max_epochs\": max_epochs,\n",
        "                \"enable_progress_bar\": False,  # Cleaner output\n",
        "            },\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        predictor = estimator.train(\n",
        "            training_data=train_ds,\n",
        "            cache_data=True,\n",
        "            shuffle_buffer_length=1000\n",
        "        )\n",
        "\n",
        "        # Generate forecasts on validation set\n",
        "        forecast_it, ts_it = make_evaluation_predictions(\n",
        "            dataset=val_ds,\n",
        "            predictor=predictor,\n",
        "            num_samples=100\n",
        "        )\n",
        "\n",
        "        forecasts = list(forecast_it)\n",
        "        tss = list(ts_it)\n",
        "\n",
        "        # Get actual values for validation period\n",
        "        actual_values = df[target_col].iloc[val_start:val_end].values\n",
        "        forecast_mean = forecasts[0].mean\n",
        "\n",
        "        # Calculate MAE as optimization metric\n",
        "        mae = np.mean(np.abs(forecast_mean - actual_values))\n",
        "\n",
        "        # Also calculate RMSE for reference\n",
        "        rmse = np.sqrt(np.mean((forecast_mean - actual_values) ** 2))\n",
        "\n",
        "        # Report intermediate value for pruning\n",
        "        trial.report(mae, step=max_epochs)\n",
        "\n",
        "        # Handle pruning\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        print(f\"Trial {trial.number}: MAE={mae:.2f}, RMSE={rmse:.2f}\")\n",
        "\n",
        "        return mae\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Trial {trial.number} failed: {str(e)}\")\n",
        "        return float('inf')  # Return worst possible score on failure\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6. RUN OPTUNA OPTIMIZATION\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🚀 STARTING HYPERPARAMETER OPTIMIZATION WITH OPTUNA\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Create study\n",
        "study = optuna.create_study(\n",
        "    direction='minimize',  # Minimize MAE\n",
        "    sampler=optuna.samplers.TPESampler(seed=42),  # Tree-structured Parzen Estimator\n",
        "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),  # Prune bad trials early\n",
        ")\n",
        "\n",
        "# Run optimization\n",
        "n_trials = 50  # Adjust based on your computational budget\n",
        "study.optimize(objective, n_trials=n_trials, timeout=None, show_progress_bar=True)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 7. ANALYZE RESULTS\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ OPTIMIZATION COMPLETE!\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"🏆 BEST HYPERPARAMETERS:\")\n",
        "print(\"-\" * 50)\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  • {key}: {value}\")\n",
        "\n",
        "print(f\"\\n📊 BEST VALIDATION MAE: {study.best_value:.2f}\")\n",
        "print(f\"📈 Total trials completed: {len(study.trials)}\")\n",
        "\n",
        "# Get statistics\n",
        "completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
        "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
        "\n",
        "print(f\"✓ Completed: {len(completed_trials)}\")\n",
        "print(f\"✗ Pruned: {len(pruned_trials)}\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 8. VISUALIZE OPTIMIZATION HISTORY\n",
        "# ---------------------------------------------------------------------------\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# Plot 1: Optimization History\n",
        "ax1 = axes[0]\n",
        "trial_numbers = [t.number for t in completed_trials]\n",
        "trial_values = [t.value for t in completed_trials]\n",
        "\n",
        "ax1.plot(trial_numbers, trial_values, 'o-', alpha=0.6, linewidth=1, markersize=4)\n",
        "ax1.axhline(y=study.best_value, color='r', linestyle='--', linewidth=2,\n",
        "           label=f'Best MAE: {study.best_value:.2f}')\n",
        "ax1.set_xlabel('Trial Number', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Validation MAE', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Optuna Optimization History', fontsize=14, fontweight='bold', pad=15)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Parameter Importance\n",
        "ax2 = axes[1]\n",
        "try:\n",
        "    importances = optuna.importance.get_param_importances(study)\n",
        "    params = list(importances.keys())\n",
        "    values = list(importances.values())\n",
        "\n",
        "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(params)))\n",
        "    bars = ax2.barh(params, values, color=colors, edgecolor='black', linewidth=1.5)\n",
        "    ax2.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Hyperparameter Importance', fontsize=14, fontweight='bold', pad=15)\n",
        "    ax2.grid(True, alpha=0.3, axis='x')\n",
        "except:\n",
        "    ax2.text(0.5, 0.5, 'Not enough trials for importance analysis',\n",
        "            ha='center', va='center', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9. TRAIN FINAL MODEL WITH BEST HYPERPARAMETERS\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🎯 TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "best_params = study.best_params\n",
        "\n",
        "# Create final datasets (use all data up to test period)\n",
        "final_train_df = df.iloc[:test_start]\n",
        "\n",
        "if selected_features:\n",
        "    final_train_ds = PandasDataset(final_train_df, target=target_col,\n",
        "                                   feat_dynamic_real=selected_features, freq=\"W-SUN\")\n",
        "    test_ds = PandasDataset(test_df, target=target_col,\n",
        "                           feat_dynamic_real=selected_features, freq=\"W-SUN\")\n",
        "else:\n",
        "    final_train_ds = PandasDataset(final_train_df, target=target_col, freq=\"W-SUN\")\n",
        "    test_ds = PandasDataset(test_df, target=target_col, freq=\"W-SUN\")\n",
        "\n",
        "# Create final estimator\n",
        "final_estimator = LagLlamaEstimator(\n",
        "    ckpt_path=ckpt_path,\n",
        "    prediction_length=prediction_length,\n",
        "    context_length=best_params['context_length'],\n",
        "    n_layer=base_estimator_args[\"n_layer\"],\n",
        "    n_embd_per_head=base_estimator_args[\"n_embd_per_head\"],\n",
        "    n_head=base_estimator_args[\"n_head\"],\n",
        "    scaling=base_estimator_args[\"scaling\"],\n",
        "    time_feat=base_estimator_args[\"time_feat\"],\n",
        "    aug_prob=best_params['aug_prob'],\n",
        "    batch_size=best_params['batch_size'],\n",
        "    num_parallel_samples=100,\n",
        "    lr=best_params['learning_rate'],\n",
        "    trainer_kwargs={\n",
        "        \"accelerator\": device,\n",
        "        \"max_epochs\": best_params['max_epochs'],\n",
        "    },\n",
        ")\n",
        "\n",
        "# Train final model\n",
        "final_predictor = final_estimator.train(\n",
        "    training_data=final_train_ds,\n",
        "    cache_data=True,\n",
        "    shuffle_buffer_length=1000\n",
        ")\n",
        "\n",
        "# Generate final forecasts\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,\n",
        "    predictor=final_predictor,\n",
        "    num_samples=100\n",
        ")\n",
        "\n",
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)\n",
        "\n",
        "# Get actual test values\n",
        "actual_values = df[target_col].iloc[test_start:test_start + prediction_length].values\n",
        "forecast_mean = forecasts[0].mean\n",
        "\n",
        "# Calculate final metrics\n",
        "mae = np.mean(np.abs(forecast_mean - actual_values))\n",
        "rmse = np.sqrt(np.mean((forecast_mean - actual_values) ** 2))\n",
        "mape = np.mean(np.abs((actual_values - forecast_mean) / actual_values)) * 100\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 10. PLOT FINAL RESULTS\n",
        "# ---------------------------------------------------------------------------\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Forecast with Historical Context\n",
        "ts_entry = tss[0]\n",
        "ts_index = ts_entry[-100:].index.to_timestamp()\n",
        "ts_values = ts_entry[-100:].values\n",
        "\n",
        "ax1.plot(ts_index, ts_values, label=\"Historical Data\", linewidth=2,\n",
        "         color='#2E86AB', marker='o', markersize=3, alpha=0.8)\n",
        "\n",
        "forecast_entry = forecasts[0]\n",
        "forecast_index = forecast_entry.index.to_timestamp()\n",
        "actual_index = df.index[test_start:test_start + prediction_length]\n",
        "\n",
        "ax1.plot(actual_index, actual_values, label=\"Actual (Test)\",\n",
        "         linewidth=2.5, color='#E63946', marker='o', markersize=5)\n",
        "\n",
        "ax1.plot(forecast_index, forecast_mean, label=\"Optimized Forecast\",\n",
        "         linewidth=2.5, color='#06A77D', marker='s', markersize=4, linestyle='--')\n",
        "\n",
        "q05 = forecast_entry.quantile('0.05')\n",
        "q95 = forecast_entry.quantile('0.95')\n",
        "ax1.fill_between(forecast_index, q05, q95, alpha=0.2, color='#06A77D',\n",
        "                 label='90% Prediction Interval')\n",
        "\n",
        "ax1.axvline(x=forecast_index[0], color='red', linestyle='--',\n",
        "           linewidth=1.5, alpha=0.7, label='Forecast Start')\n",
        "\n",
        "ax1.set_title(f\"Optimized Model: Weekly Intake Forecast vs Actual\\n\"\n",
        "             f\"MAE: {mae:.2f} | RMSE: {rmse:.2f} | MAPE: {mape:.1f}%\",\n",
        "             fontsize=14, fontweight='bold', pad=15)\n",
        "ax1.set_xlabel(\"Date\", fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel(\"Number of People\", fontsize=12, fontweight='bold')\n",
        "ax1.legend(fontsize=9, loc='best', framealpha=0.9)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
        "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Plot 2: Forecast Error Analysis\n",
        "errors = forecast_mean - actual_values\n",
        "weeks = np.arange(1, prediction_length + 1)\n",
        "\n",
        "ax2.bar(weeks, errors, color=['#06A77D' if e >= 0 else '#E63946' for e in errors],\n",
        "       alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "ax2.set_title(\"Forecast Error by Week\", fontsize=14, fontweight='bold', pad=15)\n",
        "ax2.set_xlabel(\"Week Ahead\", fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel(\"Error (Forecast - Actual)\", fontsize=12, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
        "ax2.set_xticks(weeks)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 11. FINAL SUMMARY\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ FINAL MODEL EVALUATION (TEST SET)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n📊 ACCURACY METRICS:\")\n",
        "print(f\"  • MAE (Mean Absolute Error):        {mae:.2f}\")\n",
        "print(f\"  • RMSE (Root Mean Squared Error):   {rmse:.2f}\")\n",
        "print(f\"  • MAPE (Mean Absolute % Error):     {mape:.1f}%\")\n",
        "\n",
        "print(f\"\\n🏆 BEST HYPERPARAMETERS USED:\")\n",
        "for key, value in best_params.items():\n",
        "    print(f\"  • {key}: {value}\")\n",
        "\n",
        "print(f\"\\n📋 WEEK-BY-WEEK COMPARISON:\")\n",
        "print(f\"{'Week':<6} {'Actual':<10} {'Forecast':<10} {'Error':<10} {'% Error':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for i, (actual, pred) in enumerate(zip(actual_values, forecast_mean), 1):\n",
        "    error = pred - actual\n",
        "    pct_error = (error / actual) * 100\n",
        "    print(f\"{i:<6} {actual:<10.2f} {pred:<10.2f} {error:<10.2f} {pct_error:<10.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Save best parameters to file\n",
        "import json\n",
        "with open('best_hyperparameters.json', 'w') as f:\n",
        "    json.dump(best_params, f, indent=2)\n",
        "print(\"\\n💾 Best hyperparameters saved to 'best_hyperparameters.json'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a1e23e37d97f42d692be2247ff1a019a",
            "6a4ece866f7b4dc8a663605433c4841f",
            "43f5a674fac14344b443c39da0bffd91",
            "a3827188af4d4ab6ba4fad6855efa127",
            "35747ec32f184cb88af9bcbdb55b9492",
            "72f6acab203a4a029905a256946d0dda",
            "fb775fd5c76d42a19c3f894a0e35cae7",
            "f7afa295c7aa4faaa6b8fe4919ce28b8",
            "18751f34953b4d8aa1ea151574d3966a",
            "8efecb26c9f148ae9b4d35751080dce6",
            "93901f072672411d910702ac1828f071",
            "2658e67458f54305892976a4abaec7d6",
            "717d4fb0105b467ba2d5439eeb19a864",
            "c05b09efc94149e290b2b4ecadc06877",
            "3de30e323c4a42dc84721ce455e11f5f",
            "3d1b4226fde94c3db00cdb9bd99a2e98",
            "38a4e932c28a4f4fabe6e9898d048b5b",
            "a8f21a0285c949858720eb0265e9045c",
            "2873a70a70634a729232a74ed4edf967",
            "c8cda3099e9f428581c26e941c74b9c4",
            "3dbaefcb252543f6b8cdc255129f56ca",
            "02cac27444574ad7b2249d17ff2d0599"
          ]
        },
        "id": "PM4CSQqTHeSa",
        "outputId": "ddfe2c8f-bccf-490c-d3c4-12aefaff7091"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ torch.load patched successfully\n",
            "✓ Data Loaded. Shape: (242, 44)\n",
            "✓ Date range: 2021-03-07 00:00:00 to 2025-10-19 00:00:00\n",
            "\n",
            "✓ Using 11 exogenous features\n",
            "\n",
            "✓ Train dataset: 218 weeks (up to 2025-05-04 00:00:00)\n",
            "✓ Validation period: weeks 218 to 230\n",
            "✓ Test period: last 12 weeks\n",
            "\n",
            "Downloading Lag-Llama checkpoint...\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "Downloading 'lag-llama.ckpt' to '.cache/huggingface/download/59Iq1KnnyJzBevZl6u7vR_lVuAs=.b5a5c4b8a0cfe9b81bdac35ed5d88b5033cd119b5206c28e9cd67c4b45fb2c96.incomplete'\n",
            "lag-llama.ckpt: 100% 29.5M/29.5M [00:00<00:00, 40.5MB/s]\n",
            "Download complete. Moving file to lag-llama.ckpt\n",
            "lag-llama.ckpt\n",
            "✓ Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-24 02:09:36,878] A new study created in memory with name: no-name-80a280b1-8964-44f3-a211-574ed5337d17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "🚀 STARTING HYPERPARAMETER OPTIMIZATION WITH OPTUNA\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1e23e37d97f42d692be2247ff1a019a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "WARNING: Missing logger folder: /content/lag-llama/lightning_logs\n",
            "WARNING:lightning.pytorch.loggers.tensorboard:Missing logger folder: /content/lag-llama/lightning_logs\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.18796 (best 0.18796), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.18796 (best 0.18796), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.17916 (best -0.17916), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.17916 (best -0.17916), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.40609 (best -0.40609), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.40609 (best -0.40609), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.54719 (best -0.54719), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.54719 (best -0.54719), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.89620 (best -0.89620), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.89620 (best -0.89620), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.02300 (best -1.02300), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.02300 (best -1.02300), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.11591 (best -1.11591), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.11591 (best -1.11591), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.20361 (best -1.20361), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.20361 (best -1.20361), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.40208 (best -1.40208), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.40208 (best -1.40208), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.40600 (best -1.40600), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.40600 (best -1.40600), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.42775 (best -1.42775), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.42775 (best -1.42775), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -1.43993 (best -1.43993), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -1.43993 (best -1.43993), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' reached -1.57716 (best -1.57716), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' reached -1.57716 (best -1.57716), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' reached -1.60539 (best -1.60539), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' reached -1.60539 (best -1.60539), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' reached -1.81832 (best -1.81832), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' reached -1.81832 (best -1.81832), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' reached -1.83173 (best -1.83173), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=60-step=3050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' reached -1.83173 (best -1.83173), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=60-step=3050.ckpt' as top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' reached -1.84949 (best -1.84949), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=64-step=3250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' reached -1.84949 (best -1.84949), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=64-step=3250.ckpt' as top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' reached -1.98039 (best -1.98039), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=68-step=3450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' reached -1.98039 (best -1.98039), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=68-step=3450.ckpt' as top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 79, global step 4000: 'train_loss' reached -2.00270 (best -2.00270), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=79-step=4000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 79, global step 4000: 'train_loss' reached -2.00270 (best -2.00270), saving model to '/content/lag-llama/lightning_logs/version_0/checkpoints/epoch=79-step=4000.ckpt' as top 1\n",
            "INFO: Epoch 80, global step 4050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 80, global step 4050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 81, global step 4100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 81, global step 4100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 82, global step 4150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 82, global step 4150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 83, global step 4200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 83, global step 4200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 84, global step 4250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 84, global step 4250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 85, global step 4300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 85, global step 4300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 86, global step 4350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 86, global step 4350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 87, global step 4400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 87, global step 4400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 88, global step 4450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 88, global step 4450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 89, global step 4500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 89, global step 4500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=90` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=90` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0: MAE=3.00, RMSE=3.40\n",
            "[I 2025-11-24 02:12:30,727] Trial 0 finished with value: 3.0023467540740967 and parameters: {'context_length': 32, 'batch_size': 8, 'max_epochs': 90, 'learning_rate': 0.00015930522616241006, 'aug_prob': 0.21242177333881365}. Best is trial 0 with value: 3.0023467540740967.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.15832 (best 0.15832), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.15832 (best 0.15832), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.50806 (best -0.50806), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.50806 (best -0.50806), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.79333 (best -0.79333), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.79333 (best -0.79333), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -1.05750 (best -1.05750), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -1.05750 (best -1.05750), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -1.19317 (best -1.19317), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -1.19317 (best -1.19317), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.34022 (best -1.34022), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.34022 (best -1.34022), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.58146 (best -1.58146), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.58146 (best -1.58146), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.70946 (best -1.70946), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.70946 (best -1.70946), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.94627 (best -1.94627), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.94627 (best -1.94627), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -2.00317 (best -2.00317), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -2.00317 (best -2.00317), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -2.06915 (best -2.06915), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -2.06915 (best -2.06915), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -2.11705 (best -2.11705), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -2.11705 (best -2.11705), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -2.21288 (best -2.21288), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -2.21288 (best -2.21288), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' reached -2.42438 (best -2.42438), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' reached -2.42438 (best -2.42438), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' reached -2.54526 (best -2.54526), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=37-step=1900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' reached -2.54526 (best -2.54526), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=37-step=1900.ckpt' as top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' reached -2.81211 (best -2.81211), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=53-step=2700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' reached -2.81211 (best -2.81211), saving model to '/content/lag-llama/lightning_logs/version_1/checkpoints/epoch=53-step=2700.ckpt' as top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1: MAE=3.94, RMSE=4.27\n",
            "[I 2025-11-24 02:14:37,992] Trial 1 finished with value: 3.9443092346191406 and parameters: {'context_length': 32, 'batch_size': 32, 'max_epochs': 60, 'learning_rate': 7.309539835912905e-05, 'aug_prob': 0.08736874205941257}. Best is trial 0 with value: 3.0023467540740967.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.11710 (best 0.11710), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.11710 (best 0.11710), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.38062 (best -0.38062), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.38062 (best -0.38062), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.64365 (best -0.64365), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.64365 (best -0.64365), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.77087 (best -0.77087), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.77087 (best -0.77087), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.94509 (best -0.94509), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.94509 (best -0.94509), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.16304 (best -1.16304), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.16304 (best -1.16304), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.23272 (best -1.23272), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.23272 (best -1.23272), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.23535 (best -1.23535), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.23535 (best -1.23535), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.47677 (best -1.47677), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.47677 (best -1.47677), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.51678 (best -1.51678), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.51678 (best -1.51678), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.58536 (best -1.58536), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.58536 (best -1.58536), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.91289 (best -1.91289), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.91289 (best -1.91289), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -1.96339 (best -1.96339), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -1.96339 (best -1.96339), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' reached -1.98950 (best -1.98950), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' reached -1.98950 (best -1.98950), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' reached -2.00224 (best -2.00224), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' reached -2.00224 (best -2.00224), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' reached -2.05543 (best -2.05543), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' reached -2.05543 (best -2.05543), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' reached -2.06962 (best -2.06962), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=33-step=1700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' reached -2.06962 (best -2.06962), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=33-step=1700.ckpt' as top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' reached -2.26382 (best -2.26382), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=40-step=2050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' reached -2.26382 (best -2.26382), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=40-step=2050.ckpt' as top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' reached -2.27370 (best -2.27370), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=48-step=2450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' reached -2.27370 (best -2.27370), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=48-step=2450.ckpt' as top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' reached -2.28966 (best -2.28966), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=50-step=2550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' reached -2.28966 (best -2.28966), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=50-step=2550.ckpt' as top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' reached -2.35854 (best -2.35854), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=52-step=2650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' reached -2.35854 (best -2.35854), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=52-step=2650.ckpt' as top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' reached -2.47693 (best -2.47693), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=59-step=3000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' reached -2.47693 (best -2.47693), saving model to '/content/lag-llama/lightning_logs/version_2/checkpoints/epoch=59-step=3000.ckpt' as top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2: MAE=4.28, RMSE=4.50\n",
            "[I 2025-11-24 02:16:33,571] Trial 2 finished with value: 4.275237083435059 and parameters: {'context_length': 16, 'batch_size': 16, 'max_epochs': 60, 'learning_rate': 0.00015304852121831474, 'aug_prob': 0.013935123815999317}. Best is trial 0 with value: 3.0023467540740967.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached -0.06094 (best -0.06094), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached -0.06094 (best -0.06094), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.51004 (best -0.51004), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.51004 (best -0.51004), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.78256 (best -0.78256), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.78256 (best -0.78256), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.16056 (best -1.16056), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.16056 (best -1.16056), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.30510 (best -1.30510), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.30510 (best -1.30510), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.40738 (best -1.40738), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.40738 (best -1.40738), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.44206 (best -1.44206), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.44206 (best -1.44206), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.60670 (best -1.60670), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.60670 (best -1.60670), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.76114 (best -1.76114), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.76114 (best -1.76114), saving model to '/content/lag-llama/lightning_logs/version_3/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3: MAE=7.57, RMSE=7.85\n",
            "[I 2025-11-24 02:17:12,182] Trial 3 finished with value: 7.567817687988281 and parameters: {'context_length': 96, 'batch_size': 8, 'max_epochs': 20, 'learning_rate': 0.000233596350262616, 'aug_prob': 0.13204574812188039}. Best is trial 0 with value: 3.0023467540740967.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached -0.12140 (best -0.12140), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached -0.12140 (best -0.12140), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.82146 (best -0.82146), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.82146 (best -0.82146), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -1.03993 (best -1.03993), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -1.03993 (best -1.03993), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -1.40238 (best -1.40238), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -1.40238 (best -1.40238), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -1.57319 (best -1.57319), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -1.57319 (best -1.57319), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.79960 (best -1.79960), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.79960 (best -1.79960), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.88407 (best -1.88407), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.88407 (best -1.88407), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -2.12926 (best -2.12926), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -2.12926 (best -2.12926), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -2.13421 (best -2.13421), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -2.13421 (best -2.13421), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -2.23887 (best -2.23887), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -2.23887 (best -2.23887), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -2.38962 (best -2.38962), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -2.38962 (best -2.38962), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -2.39079 (best -2.39079), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -2.39079 (best -2.39079), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -2.46776 (best -2.46776), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -2.46776 (best -2.46776), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' reached -2.52367 (best -2.52367), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' reached -2.52367 (best -2.52367), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' reached -2.58524 (best -2.58524), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' reached -2.58524 (best -2.58524), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' reached -2.76042 (best -2.76042), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=36-step=1850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' reached -2.76042 (best -2.76042), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=36-step=1850.ckpt' as top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' reached -2.97280 (best -2.97280), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=43-step=2200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' reached -2.97280 (best -2.97280), saving model to '/content/lag-llama/lightning_logs/version_4/checkpoints/epoch=43-step=2200.ckpt' as top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4: MAE=2.47, RMSE=2.70\n",
            "[I 2025-11-24 02:19:11,607] Trial 4 finished with value: 2.466684579849243 and parameters: {'context_length': 96, 'batch_size': 16, 'max_epochs': 60, 'learning_rate': 0.00012399967836846095, 'aug_prob': 0.05545633665765811}. Best is trial 4 with value: 2.466684579849243.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.48224 (best 0.48224), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.48224 (best 0.48224), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.10560 (best 0.10560), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.10560 (best 0.10560), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached 0.00550 (best 0.00550), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached 0.00550 (best 0.00550), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.17960 (best -0.17960), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.17960 (best -0.17960), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.32443 (best -0.32443), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.32443 (best -0.32443), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.58771 (best -0.58771), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.58771 (best -0.58771), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -0.74967 (best -0.74967), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -0.74967 (best -0.74967), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -0.78749 (best -0.78749), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -0.78749 (best -0.78749), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.05225 (best -1.05225), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.05225 (best -1.05225), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.09420 (best -1.09420), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.09420 (best -1.09420), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -1.21768 (best -1.21768), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -1.21768 (best -1.21768), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' reached -1.26385 (best -1.26385), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' reached -1.26385 (best -1.26385), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.31841 (best -1.31841), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.31841 (best -1.31841), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' reached -1.41696 (best -1.41696), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' reached -1.41696 (best -1.41696), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' reached -1.44776 (best -1.44776), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' reached -1.44776 (best -1.44776), saving model to '/content/lag-llama/lightning_logs/version_5/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5: MAE=2.07, RMSE=2.63\n",
            "[I 2025-11-24 02:20:10,325] Trial 5 finished with value: 2.0673673152923584 and parameters: {'context_length': 16, 'batch_size': 16, 'max_epochs': 30, 'learning_rate': 1.2315571723666024e-05, 'aug_prob': 0.0975990992289793}. Best is trial 5 with value: 2.0673673152923584.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.20085 (best 0.20085), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.20085 (best 0.20085), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.07016 (best -0.07016), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.07016 (best -0.07016), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.19614 (best -0.19614), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.19614 (best -0.19614), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.43402 (best -0.43402), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.43402 (best -0.43402), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.54760 (best -0.54760), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.54760 (best -0.54760), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.64948 (best -0.64948), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.64948 (best -0.64948), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -0.88645 (best -0.88645), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -0.88645 (best -0.88645), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.07350 (best -1.07350), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.07350 (best -1.07350), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.11550 (best -1.11550), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.11550 (best -1.11550), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.13223 (best -1.13223), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.13223 (best -1.13223), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.41260 (best -1.41260), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.41260 (best -1.41260), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.51644 (best -1.51644), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.51644 (best -1.51644), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' reached -1.58479 (best -1.58479), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' reached -1.58479 (best -1.58479), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' reached -1.73047 (best -1.73047), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=37-step=1900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' reached -1.73047 (best -1.73047), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=37-step=1900.ckpt' as top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' reached -1.73050 (best -1.73050), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=44-step=2250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' reached -1.73050 (best -1.73050), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=44-step=2250.ckpt' as top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' reached -1.74389 (best -1.74389), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=48-step=2450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' reached -1.74389 (best -1.74389), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=48-step=2450.ckpt' as top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' reached -1.94259 (best -1.94259), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=49-step=2500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' reached -1.94259 (best -1.94259), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=49-step=2500.ckpt' as top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' reached -1.95448 (best -1.95448), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=51-step=2600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' reached -1.95448 (best -1.95448), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=51-step=2600.ckpt' as top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' reached -2.17613 (best -2.17613), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=63-step=3200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' reached -2.17613 (best -2.17613), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=63-step=3200.ckpt' as top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' reached -2.18551 (best -2.18551), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=65-step=3300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' reached -2.18551 (best -2.18551), saving model to '/content/lag-llama/lightning_logs/version_6/checkpoints/epoch=65-step=3300.ckpt' as top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 80, global step 4050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 80, global step 4050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 81, global step 4100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 81, global step 4100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 82, global step 4150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 82, global step 4150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 83, global step 4200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 83, global step 4200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 84, global step 4250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 84, global step 4250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 85, global step 4300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 85, global step 4300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 86, global step 4350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 86, global step 4350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 87, global step 4400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 87, global step 4400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 88, global step 4450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 88, global step 4450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 89, global step 4500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 89, global step 4500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=90` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=90` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 6: MAE=1.62, RMSE=1.94\n",
            "[I 2025-11-24 02:23:07,751] Trial 6 finished with value: 1.615470051765442 and parameters: {'context_length': 64, 'batch_size': 16, 'max_epochs': 90, 'learning_rate': 1.4096175149815848e-05, 'aug_prob': 0.2960660809801552}. Best is trial 6 with value: 1.615470051765442.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached -0.05245 (best -0.05245), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached -0.05245 (best -0.05245), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.69876 (best -0.69876), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.69876 (best -0.69876), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -1.17969 (best -1.17969), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -1.17969 (best -1.17969), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -1.42196 (best -1.42196), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -1.42196 (best -1.42196), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -1.60114 (best -1.60114), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -1.60114 (best -1.60114), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -2.07446 (best -2.07446), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -2.07446 (best -2.07446), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -2.13793 (best -2.13793), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -2.13793 (best -2.13793), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -2.17967 (best -2.17967), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -2.17967 (best -2.17967), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -2.28185 (best -2.28185), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -2.28185 (best -2.28185), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -2.45800 (best -2.45800), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -2.45800 (best -2.45800), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -2.58760 (best -2.58760), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -2.58760 (best -2.58760), saving model to '/content/lag-llama/lightning_logs/version_7/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7: MAE=2.02, RMSE=2.73\n",
            "[I 2025-11-24 02:23:59,358] Trial 7 finished with value: 2.022369623184204 and parameters: {'context_length': 96, 'batch_size': 32, 'max_epochs': 20, 'learning_rate': 5.211124595788268e-05, 'aug_prob': 0.03476071785753891}. Best is trial 6 with value: 1.615470051765442.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.48298 (best 0.48298), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.48298 (best 0.48298), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.18752 (best -0.18752), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.18752 (best -0.18752), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.32690 (best -0.32690), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.32690 (best -0.32690), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.56863 (best -0.56863), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.56863 (best -0.56863), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -0.62682 (best -0.62682), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -0.62682 (best -0.62682), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -0.82107 (best -0.82107), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -0.82107 (best -0.82107), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -0.83901 (best -0.83901), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -0.83901 (best -0.83901), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.08711 (best -1.08711), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.08711 (best -1.08711), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -1.10804 (best -1.10804), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -1.10804 (best -1.10804), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.18839 (best -1.18839), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.18839 (best -1.18839), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.33830 (best -1.33830), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.33830 (best -1.33830), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -1.45053 (best -1.45053), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -1.45053 (best -1.45053), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' reached -1.49935 (best -1.49935), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=28-step=1450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' reached -1.49935 (best -1.49935), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=28-step=1450.ckpt' as top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' reached -1.55550 (best -1.55550), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' reached -1.55550 (best -1.55550), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' reached -1.74616 (best -1.74616), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=38-step=1950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' reached -1.74616 (best -1.74616), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=38-step=1950.ckpt' as top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' reached -1.75073 (best -1.75073), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=59-step=3000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' reached -1.75073 (best -1.75073), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=59-step=3000.ckpt' as top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' reached -1.89216 (best -1.89216), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=63-step=3200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' reached -1.89216 (best -1.89216), saving model to '/content/lag-llama/lightning_logs/version_8/checkpoints/epoch=63-step=3200.ckpt' as top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=70` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=70` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 8: MAE=2.57, RMSE=3.12\n",
            "[I 2025-11-24 02:26:27,266] Trial 8 finished with value: 2.5711700916290283 and parameters: {'context_length': 16, 'batch_size': 32, 'max_epochs': 70, 'learning_rate': 0.000594874681321977, 'aug_prob': 0.14166447754858477}. Best is trial 6 with value: 1.615470051765442.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.30776 (best 0.30776), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.30776 (best 0.30776), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.02939 (best 0.02939), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.02939 (best 0.02939), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.20061 (best -0.20061), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.20061 (best -0.20061), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.39159 (best -0.39159), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.39159 (best -0.39159), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.58940 (best -0.58940), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.58940 (best -0.58940), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.69366 (best -0.69366), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.69366 (best -0.69366), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.93961 (best -0.93961), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.93961 (best -0.93961), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.03638 (best -1.03638), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.03638 (best -1.03638), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.12133 (best -1.12133), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.12133 (best -1.12133), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.32125 (best -1.32125), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.32125 (best -1.32125), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.35651 (best -1.35651), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.35651 (best -1.35651), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.47635 (best -1.47635), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.47635 (best -1.47635), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.58284 (best -1.58284), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.58284 (best -1.58284), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.62599 (best -1.62599), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.62599 (best -1.62599), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.74699 (best -1.74699), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.74699 (best -1.74699), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' reached -1.91389 (best -1.91389), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' reached -1.91389 (best -1.91389), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -1.95679 (best -1.95679), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -1.95679 (best -1.95679), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' reached -2.10211 (best -2.10211), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' reached -2.10211 (best -2.10211), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' reached -2.23559 (best -2.23559), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=36-step=1850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' reached -2.23559 (best -2.23559), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=36-step=1850.ckpt' as top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' reached -2.37686 (best -2.37686), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=47-step=2400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' reached -2.37686 (best -2.37686), saving model to '/content/lag-llama/lightning_logs/version_9/checkpoints/epoch=47-step=2400.ckpt' as top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9: MAE=2.71, RMSE=2.91\n",
            "[I 2025-11-24 02:28:00,592] Trial 9 finished with value: 2.7138030529022217 and parameters: {'context_length': 64, 'batch_size': 8, 'max_epochs': 50, 'learning_rate': 1.1241862095793047e-05, 'aug_prob': 0.032367428097991334}. Best is trial 6 with value: 1.615470051765442.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.16043 (best 0.16043), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.16043 (best 0.16043), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.13841 (best -0.13841), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.13841 (best -0.13841), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.46643 (best -0.46643), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.46643 (best -0.46643), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.63147 (best -0.63147), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.63147 (best -0.63147), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.84645 (best -0.84645), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.84645 (best -0.84645), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.02773 (best -1.02773), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.02773 (best -1.02773), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.08986 (best -1.08986), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.08986 (best -1.08986), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.13484 (best -1.13484), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.13484 (best -1.13484), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.13917 (best -1.13917), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.13917 (best -1.13917), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.28273 (best -1.28273), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.28273 (best -1.28273), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.30302 (best -1.30302), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.30302 (best -1.30302), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -1.44931 (best -1.44931), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -1.44931 (best -1.44931), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.68142 (best -1.68142), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.68142 (best -1.68142), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' reached -1.71237 (best -1.71237), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' reached -1.71237 (best -1.71237), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' reached -1.88066 (best -1.88066), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' reached -1.88066 (best -1.88066), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' reached -2.26240 (best -2.26240), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=42-step=2150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' reached -2.26240 (best -2.26240), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=42-step=2150.ckpt' as top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' reached -2.36587 (best -2.36587), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=56-step=2850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' reached -2.36587 (best -2.36587), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=56-step=2850.ckpt' as top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 80, global step 4050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 80, global step 4050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 81, global step 4100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 81, global step 4100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 82, global step 4150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 82, global step 4150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 83, global step 4200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 83, global step 4200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 84, global step 4250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 84, global step 4250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 85, global step 4300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 85, global step 4300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 86, global step 4350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 86, global step 4350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 87, global step 4400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 87, global step 4400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 88, global step 4450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 88, global step 4450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 89, global step 4500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 89, global step 4500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 90, global step 4550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 90, global step 4550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 91, global step 4600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 91, global step 4600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 92, global step 4650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 92, global step 4650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 93, global step 4700: 'train_loss' reached -2.48866 (best -2.48866), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=93-step=4700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 93, global step 4700: 'train_loss' reached -2.48866 (best -2.48866), saving model to '/content/lag-llama/lightning_logs/version_10/checkpoints/epoch=93-step=4700.ckpt' as top 1\n",
            "INFO: Epoch 94, global step 4750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 94, global step 4750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 95, global step 4800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 95, global step 4800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 96, global step 4850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 96, global step 4850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 97, global step 4900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 97, global step 4900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 98, global step 4950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 98, global step 4950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 99, global step 5000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 99, global step 5000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10: MAE=2.49, RMSE=2.84\n",
            "[I 2025-11-24 02:31:18,455] Trial 10 finished with value: 2.492443799972534 and parameters: {'context_length': 64, 'batch_size': 16, 'max_epochs': 100, 'learning_rate': 2.9207080600637215e-05, 'aug_prob': 0.29516331250156236}. Best is trial 6 with value: 1.615470051765442.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.08593 (best 0.08593), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.08593 (best 0.08593), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.26613 (best -0.26613), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.26613 (best -0.26613), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.70993 (best -0.70993), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.70993 (best -0.70993), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -1.02221 (best -1.02221), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -1.02221 (best -1.02221), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.12151 (best -1.12151), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.12151 (best -1.12151), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.58754 (best -1.58754), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.58754 (best -1.58754), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.83544 (best -1.83544), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.83544 (best -1.83544), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.89577 (best -1.89577), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.89577 (best -1.89577), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -2.08836 (best -2.08836), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -2.08836 (best -2.08836), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -2.14116 (best -2.14116), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -2.14116 (best -2.14116), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' reached -2.16927 (best -2.16927), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' reached -2.16927 (best -2.16927), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' reached -2.33415 (best -2.33415), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' reached -2.33415 (best -2.33415), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' reached -2.44818 (best -2.44818), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' reached -2.44818 (best -2.44818), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' reached -2.56579 (best -2.56579), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=58-step=2950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' reached -2.56579 (best -2.56579), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=58-step=2950.ckpt' as top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 74, global step 3750: 'train_loss' reached -2.75622 (best -2.75622), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=74-step=3750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 74, global step 3750: 'train_loss' reached -2.75622 (best -2.75622), saving model to '/content/lag-llama/lightning_logs/version_11/checkpoints/epoch=74-step=3750.ckpt' as top 1\n",
            "INFO: Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=80` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=80` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 11: MAE=2.22, RMSE=2.65\n",
            "[I 2025-11-24 02:34:41,486] Trial 11 finished with value: 2.21585750579834 and parameters: {'context_length': 96, 'batch_size': 32, 'max_epochs': 80, 'learning_rate': 3.2930239367756246e-05, 'aug_prob': 0.2215216634175063}. Best is trial 6 with value: 1.615470051765442.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.11487 (best 0.11487), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.11487 (best 0.11487), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.25108 (best -0.25108), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.25108 (best -0.25108), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.60928 (best -0.60928), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.60928 (best -0.60928), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.79292 (best -0.79292), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.79292 (best -0.79292), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.86742 (best -0.86742), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.86742 (best -0.86742), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.14865 (best -1.14865), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.14865 (best -1.14865), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.25810 (best -1.25810), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.25810 (best -1.25810), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.27453 (best -1.27453), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.27453 (best -1.27453), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.41341 (best -1.41341), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.41341 (best -1.41341), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.57917 (best -1.57917), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.57917 (best -1.57917), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -1.66030 (best -1.66030), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -1.66030 (best -1.66030), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.71210 (best -1.71210), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.71210 (best -1.71210), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' reached -1.76449 (best -1.76449), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' reached -1.76449 (best -1.76449), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' reached -1.77943 (best -1.77943), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=33-step=1700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' reached -1.77943 (best -1.77943), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=33-step=1700.ckpt' as top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' reached -1.82182 (best -1.82182), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' reached -1.82182 (best -1.82182), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' reached -2.07636 (best -2.07636), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' reached -2.07636 (best -2.07636), saving model to '/content/lag-llama/lightning_logs/version_12/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 12: MAE=1.82, RMSE=2.26\n",
            "[I 2025-11-24 02:36:08,080] Trial 12 finished with value: 1.8228265047073364 and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 40, 'learning_rate': 3.575306158275995e-05, 'aug_prob': 0.29820058655830717}. Best is trial 6 with value: 1.615470051765442.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.15411 (best 0.15411), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.15411 (best 0.15411), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.03936 (best -0.03936), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.03936 (best -0.03936), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.41916 (best -0.41916), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.41916 (best -0.41916), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.84524 (best -0.84524), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.84524 (best -0.84524), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.92007 (best -0.92007), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.92007 (best -0.92007), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.04576 (best -1.04576), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.04576 (best -1.04576), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.14457 (best -1.14457), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.14457 (best -1.14457), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.19836 (best -1.19836), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.19836 (best -1.19836), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.35489 (best -1.35489), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.35489 (best -1.35489), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.43498 (best -1.43498), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.43498 (best -1.43498), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.64004 (best -1.64004), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.64004 (best -1.64004), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' reached -1.81859 (best -1.81859), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' reached -1.81859 (best -1.81859), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' reached -1.90188 (best -1.90188), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' reached -1.90188 (best -1.90188), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' reached -1.96525 (best -1.96525), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=31-step=1600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' reached -1.96525 (best -1.96525), saving model to '/content/lag-llama/lightning_logs/version_13/checkpoints/epoch=31-step=1600.ckpt' as top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 13: MAE=1.48, RMSE=1.87\n",
            "[I 2025-11-24 02:37:36,198] Trial 13 finished with value: 1.481684684753418 and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 40, 'learning_rate': 2.1485023652542962e-05, 'aug_prob': 0.2917825816913715}. Best is trial 13 with value: 1.481684684753418.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.25551 (best 0.25551), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.25551 (best 0.25551), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.00173 (best 0.00173), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.00173 (best 0.00173), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.26961 (best -0.26961), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.26961 (best -0.26961), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.46768 (best -0.46768), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.46768 (best -0.46768), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.57162 (best -0.57162), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.57162 (best -0.57162), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.81361 (best -0.81361), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.81361 (best -0.81361), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -0.85168 (best -0.85168), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -0.85168 (best -0.85168), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.07015 (best -1.07015), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.07015 (best -1.07015), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.17479 (best -1.17479), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.17479 (best -1.17479), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.33039 (best -1.33039), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.33039 (best -1.33039), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.38420 (best -1.38420), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.38420 (best -1.38420), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -1.66808 (best -1.66808), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -1.66808 (best -1.66808), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' reached -2.00688 (best -2.00688), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=28-step=1450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' reached -2.00688 (best -2.00688), saving model to '/content/lag-llama/lightning_logs/version_14/checkpoints/epoch=28-step=1450.ckpt' as top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 14: MAE=1.58, RMSE=1.92\n",
            "[I 2025-11-24 02:38:55,496] Trial 14 finished with value: 1.5815638303756714 and parameters: {'context_length': 64, 'batch_size': 16, 'max_epochs': 40, 'learning_rate': 1.59658057534215e-05, 'aug_prob': 0.24323364852634968}. Best is trial 13 with value: 1.481684684753418.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.16755 (best 0.16755), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.16755 (best 0.16755), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.03003 (best -0.03003), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.03003 (best -0.03003), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.43352 (best -0.43352), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.43352 (best -0.43352), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.57802 (best -0.57802), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.57802 (best -0.57802), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.67669 (best -0.67669), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.67669 (best -0.67669), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.98532 (best -0.98532), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.98532 (best -0.98532), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.02803 (best -1.02803), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.02803 (best -1.02803), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.28011 (best -1.28011), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.28011 (best -1.28011), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.34054 (best -1.34054), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.34054 (best -1.34054), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.44124 (best -1.44124), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.44124 (best -1.44124), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.60220 (best -1.60220), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.60220 (best -1.60220), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' reached -1.72961 (best -1.72961), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' reached -1.72961 (best -1.72961), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.79457 (best -1.79457), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.79457 (best -1.79457), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -1.91599 (best -1.91599), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -1.91599 (best -1.91599), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' reached -2.02735 (best -2.02735), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' reached -2.02735 (best -2.02735), saving model to '/content/lag-llama/lightning_logs/version_15/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 15: MAE=1.42, RMSE=1.79\n",
            "[I 2025-11-24 02:40:15,975] Trial 15 finished with value: 1.42127525806427 and parameters: {'context_length': 64, 'batch_size': 16, 'max_epochs': 40, 'learning_rate': 2.133531957539268e-05, 'aug_prob': 0.23559723078115413}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.13529 (best 0.13529), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.13529 (best 0.13529), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.20799 (best -0.20799), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.20799 (best -0.20799), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.49945 (best -0.49945), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.49945 (best -0.49945), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.74679 (best -0.74679), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.74679 (best -0.74679), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.79361 (best -0.79361), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.79361 (best -0.79361), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.03960 (best -1.03960), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.03960 (best -1.03960), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.08195 (best -1.08195), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.08195 (best -1.08195), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.14627 (best -1.14627), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.14627 (best -1.14627), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.31387 (best -1.31387), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.31387 (best -1.31387), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.37433 (best -1.37433), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.37433 (best -1.37433), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.48407 (best -1.48407), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.48407 (best -1.48407), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.75687 (best -1.75687), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.75687 (best -1.75687), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.80595 (best -1.80595), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.80595 (best -1.80595), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' reached -1.89434 (best -1.89434), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' reached -1.89434 (best -1.89434), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' reached -1.94085 (best -1.94085), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' reached -1.94085 (best -1.94085), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' reached -1.98371 (best -1.98371), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' reached -1.98371 (best -1.98371), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' reached -2.12896 (best -2.12896), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' reached -2.12896 (best -2.12896), saving model to '/content/lag-llama/lightning_logs/version_16/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 16: MAE=1.51, RMSE=1.87\n",
            "[I 2025-11-24 02:41:43,167] Trial 16 finished with value: 1.5108050107955933 and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 40, 'learning_rate': 2.7762130504113913e-05, 'aug_prob': 0.2531940305133584}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.10593 (best 0.10593), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.10593 (best 0.10593), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.18626 (best -0.18626), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.18626 (best -0.18626), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.26596 (best -0.26596), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.26596 (best -0.26596), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.72018 (best -0.72018), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.72018 (best -0.72018), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.91293 (best -0.91293), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.91293 (best -0.91293), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.12079 (best -1.12079), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.12079 (best -1.12079), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.24520 (best -1.24520), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.24520 (best -1.24520), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.31874 (best -1.31874), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.31874 (best -1.31874), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.61593 (best -1.61593), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.61593 (best -1.61593), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.64310 (best -1.64310), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.64310 (best -1.64310), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -1.84567 (best -1.84567), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -1.84567 (best -1.84567), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -1.87728 (best -1.87728), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -1.87728 (best -1.87728), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' reached -2.02542 (best -2.02542), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' reached -2.02542 (best -2.02542), saving model to '/content/lag-llama/lightning_logs/version_17/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 17: MAE=1.62, RMSE=1.92\n",
            "[I 2025-11-24 02:42:43,695] Trial 17 finished with value: 1.6182117462158203 and parameters: {'context_length': 64, 'batch_size': 16, 'max_epochs': 30, 'learning_rate': 2.191320154430063e-05, 'aug_prob': 0.1919063153597995}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.05274 (best 0.05274), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.05274 (best 0.05274), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.52908 (best -0.52908), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.52908 (best -0.52908), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.85309 (best -0.85309), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.85309 (best -0.85309), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.99942 (best -0.99942), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.99942 (best -0.99942), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -1.39568 (best -1.39568), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -1.39568 (best -1.39568), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.65328 (best -1.65328), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.65328 (best -1.65328), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.87776 (best -1.87776), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.87776 (best -1.87776), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -2.22938 (best -2.22938), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -2.22938 (best -2.22938), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' reached -2.32001 (best -2.32001), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' reached -2.32001 (best -2.32001), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' reached -2.35873 (best -2.35873), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=31-step=1600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' reached -2.35873 (best -2.35873), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=31-step=1600.ckpt' as top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' reached -2.67237 (best -2.67237), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=38-step=1950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' reached -2.67237 (best -2.67237), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=38-step=1950.ckpt' as top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' reached -2.68721 (best -2.68721), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=46-step=2350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' reached -2.68721 (best -2.68721), saving model to '/content/lag-llama/lightning_logs/version_18/checkpoints/epoch=46-step=2350.ckpt' as top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 18: MAE=2.23, RMSE=2.65\n",
            "[I 2025-11-24 02:44:31,137] Trial 18 finished with value: 2.234238862991333 and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 50, 'learning_rate': 5.887566238745411e-05, 'aug_prob': 0.180910370312246}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.67542 (best 0.67542), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.67542 (best 0.67542), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.24639 (best 0.24639), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.24639 (best 0.24639), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached 0.22946 (best 0.22946), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached 0.22946 (best 0.22946), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.17366 (best -0.17366), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.17366 (best -0.17366), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.34535 (best -0.34535), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.34535 (best -0.34535), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -0.70771 (best -0.70771), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -0.70771 (best -0.70771), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -0.71462 (best -0.71462), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -0.71462 (best -0.71462), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -0.73569 (best -0.73569), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -0.73569 (best -0.73569), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' reached -0.79794 (best -0.79794), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' reached -0.79794 (best -0.79794), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -0.93629 (best -0.93629), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -0.93629 (best -0.93629), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' reached -1.02785 (best -1.02785), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' reached -1.02785 (best -1.02785), saving model to '/content/lag-llama/lightning_logs/version_19/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 19 failed: \n",
            "[I 2025-11-24 02:45:28,259] Trial 19 finished with value: inf and parameters: {'context_length': 32, 'batch_size': 8, 'max_epochs': 30, 'learning_rate': 0.0007501054462493084, 'aug_prob': 0.26212333077164623}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.15096 (best 0.15096), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.15096 (best 0.15096), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.13167 (best -0.13167), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.13167 (best -0.13167), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.43295 (best -0.43295), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.43295 (best -0.43295), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.73587 (best -0.73587), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.73587 (best -0.73587), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.87891 (best -0.87891), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.87891 (best -0.87891), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.05681 (best -1.05681), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.05681 (best -1.05681), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.19768 (best -1.19768), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.19768 (best -1.19768), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.35530 (best -1.35530), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.35530 (best -1.35530), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.38778 (best -1.38778), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.38778 (best -1.38778), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.47581 (best -1.47581), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.47581 (best -1.47581), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.84707 (best -1.84707), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.84707 (best -1.84707), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.98295 (best -1.98295), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.98295 (best -1.98295), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' reached -2.30020 (best -2.30020), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=40-step=2050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' reached -2.30020 (best -2.30020), saving model to '/content/lag-llama/lightning_logs/version_20/checkpoints/epoch=40-step=2050.ckpt' as top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 20: MAE=1.53, RMSE=1.88\n",
            "[I 2025-11-24 02:47:17,052] Trial 20 finished with value: 1.5332859754562378 and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 50, 'learning_rate': 2.100054494960082e-05, 'aug_prob': 0.26758680289473213}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.05569 (best 0.05569), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.05569 (best 0.05569), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.43552 (best -0.43552), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.43552 (best -0.43552), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.87146 (best -0.87146), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.87146 (best -0.87146), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.89669 (best -0.89669), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.89669 (best -0.89669), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.95199 (best -0.95199), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.95199 (best -0.95199), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.01604 (best -1.01604), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.01604 (best -1.01604), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.21252 (best -1.21252), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.21252 (best -1.21252), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.25631 (best -1.25631), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.25631 (best -1.25631), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.52948 (best -1.52948), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.52948 (best -1.52948), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.56937 (best -1.56937), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.56937 (best -1.56937), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -1.58583 (best -1.58583), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -1.58583 (best -1.58583), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -2.01208 (best -2.01208), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -2.01208 (best -2.01208), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' reached -2.03369 (best -2.03369), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=31-step=1600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' reached -2.03369 (best -2.03369), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=31-step=1600.ckpt' as top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' reached -2.10863 (best -2.10863), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' reached -2.10863 (best -2.10863), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' reached -2.16936 (best -2.16936), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=36-step=1850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' reached -2.16936 (best -2.16936), saving model to '/content/lag-llama/lightning_logs/version_21/checkpoints/epoch=36-step=1850.ckpt' as top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 21 failed: \n",
            "[I 2025-11-24 02:48:43,511] Trial 21 finished with value: inf and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 40, 'learning_rate': 4.315518759625504e-05, 'aug_prob': 0.2539396059684739}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.21802 (best 0.21802), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.21802 (best 0.21802), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.14224 (best -0.14224), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.14224 (best -0.14224), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.45113 (best -0.45113), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.45113 (best -0.45113), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.72376 (best -0.72376), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.72376 (best -0.72376), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.93562 (best -0.93562), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.93562 (best -0.93562), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.06813 (best -1.06813), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.06813 (best -1.06813), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.17515 (best -1.17515), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.17515 (best -1.17515), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.19592 (best -1.19592), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.19592 (best -1.19592), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.38338 (best -1.38338), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.38338 (best -1.38338), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.47802 (best -1.47802), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.47802 (best -1.47802), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.74610 (best -1.74610), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.74610 (best -1.74610), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.80666 (best -1.80666), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.80666 (best -1.80666), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.91518 (best -1.91518), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.91518 (best -1.91518), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' reached -2.07737 (best -2.07737), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' reached -2.07737 (best -2.07737), saving model to '/content/lag-llama/lightning_logs/version_22/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 22 failed: \n",
            "[I 2025-11-24 02:50:10,647] Trial 22 finished with value: inf and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 40, 'learning_rate': 2.1679482060209042e-05, 'aug_prob': 0.22634455296530437}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.09535 (best 0.09535), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.09535 (best 0.09535), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.52280 (best -0.52280), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.52280 (best -0.52280), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -1.00919 (best -1.00919), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -1.00919 (best -1.00919), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -1.15662 (best -1.15662), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -1.15662 (best -1.15662), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -1.18868 (best -1.18868), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -1.18868 (best -1.18868), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.41107 (best -1.41107), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.41107 (best -1.41107), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.50123 (best -1.50123), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.50123 (best -1.50123), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.65798 (best -1.65798), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.65798 (best -1.65798), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.94204 (best -1.94204), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.94204 (best -1.94204), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -2.03882 (best -2.03882), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -2.03882 (best -2.03882), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -2.03897 (best -2.03897), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -2.03897 (best -2.03897), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -2.30600 (best -2.30600), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -2.30600 (best -2.30600), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' reached -2.39504 (best -2.39504), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=38-step=1950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' reached -2.39504 (best -2.39504), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=38-step=1950.ckpt' as top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' reached -2.39645 (best -2.39645), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=42-step=2150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' reached -2.39645 (best -2.39645), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=42-step=2150.ckpt' as top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' reached -2.69622 (best -2.69622), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=46-step=2350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' reached -2.69622 (best -2.69622), saving model to '/content/lag-llama/lightning_logs/version_23/checkpoints/epoch=46-step=2350.ckpt' as top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 23 failed: \n",
            "[I 2025-11-24 02:51:58,359] Trial 23 finished with value: inf and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 50, 'learning_rate': 7.450744435901373e-05, 'aug_prob': 0.18822594798449024}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.19506 (best 0.19506), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.19506 (best 0.19506), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.15534 (best -0.15534), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.15534 (best -0.15534), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.50558 (best -0.50558), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.50558 (best -0.50558), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.67543 (best -0.67543), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.67543 (best -0.67543), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -1.15617 (best -1.15617), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -1.15617 (best -1.15617), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.18188 (best -1.18188), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.18188 (best -1.18188), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.21705 (best -1.21705), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.21705 (best -1.21705), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.50190 (best -1.50190), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.50190 (best -1.50190), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -1.58030 (best -1.58030), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -1.58030 (best -1.58030), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' reached -1.65560 (best -1.65560), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' reached -1.65560 (best -1.65560), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.66214 (best -1.66214), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.66214 (best -1.66214), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -1.89657 (best -1.89657), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -1.89657 (best -1.89657), saving model to '/content/lag-llama/lightning_logs/version_24/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 24: MAE=1.50, RMSE=1.86\n",
            "[I 2025-11-24 02:53:03,496] Trial 24 finished with value: 1.502907395362854 and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 30, 'learning_rate': 2.6447907092265575e-05, 'aug_prob': 0.2723036468053366}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.22113 (best 0.22113), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.22113 (best 0.22113), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.00439 (best 0.00439), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.00439 (best 0.00439), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.07157 (best -0.07157), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.07157 (best -0.07157), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.34570 (best -0.34570), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.34570 (best -0.34570), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.49618 (best -0.49618), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.49618 (best -0.49618), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.66243 (best -0.66243), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.66243 (best -0.66243), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -0.66923 (best -0.66923), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -0.66923 (best -0.66923), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -0.86436 (best -0.86436), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -0.86436 (best -0.86436), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.03590 (best -1.03590), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.03590 (best -1.03590), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.04526 (best -1.04526), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.04526 (best -1.04526), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.31507 (best -1.31507), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.31507 (best -1.31507), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.34173 (best -1.34173), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.34173 (best -1.34173), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.35247 (best -1.35247), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.35247 (best -1.35247), saving model to '/content/lag-llama/lightning_logs/version_25/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 25: MAE=2.40, RMSE=2.60\n",
            "[I 2025-11-24 02:53:44,645] Trial 25 finished with value: 2.4014499187469482 and parameters: {'context_length': 64, 'batch_size': 16, 'max_epochs': 20, 'learning_rate': 1.0278683039394949e-05, 'aug_prob': 0.2810909681458509}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.20176 (best 0.20176), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.20176 (best 0.20176), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.15258 (best -0.15258), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.15258 (best -0.15258), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.38910 (best -0.38910), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.38910 (best -0.38910), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.58151 (best -0.58151), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.58151 (best -0.58151), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.63632 (best -0.63632), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.63632 (best -0.63632), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.85520 (best -0.85520), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.85520 (best -0.85520), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.20424 (best -1.20424), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.20424 (best -1.20424), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.28252 (best -1.28252), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.28252 (best -1.28252), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.52390 (best -1.52390), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.52390 (best -1.52390), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.67557 (best -1.67557), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.67557 (best -1.67557), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -1.85882 (best -1.85882), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -1.85882 (best -1.85882), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -1.95984 (best -1.95984), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -1.95984 (best -1.95984), saving model to '/content/lag-llama/lightning_logs/version_26/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 26: MAE=1.65, RMSE=1.97\n",
            "[I 2025-11-24 02:54:50,003] Trial 26 finished with value: 1.6487466096878052 and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 30, 'learning_rate': 1.773153844095825e-05, 'aug_prob': 0.2404442848888683}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.21193 (best 0.21193), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.21193 (best 0.21193), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.33954 (best -0.33954), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.33954 (best -0.33954), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.60306 (best -0.60306), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.60306 (best -0.60306), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.74693 (best -0.74693), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.74693 (best -0.74693), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.87173 (best -0.87173), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.87173 (best -0.87173), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.95745 (best -0.95745), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.95745 (best -0.95745), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.11836 (best -1.11836), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.11836 (best -1.11836), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.21516 (best -1.21516), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.21516 (best -1.21516), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.22723 (best -1.22723), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.22723 (best -1.22723), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.34864 (best -1.34864), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.34864 (best -1.34864), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -1.49728 (best -1.49728), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -1.49728 (best -1.49728), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' reached -1.68044 (best -1.68044), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' reached -1.68044 (best -1.68044), saving model to '/content/lag-llama/lightning_logs/version_27/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 27 failed: \n",
            "[I 2025-11-24 02:55:55,852] Trial 27 finished with value: inf and parameters: {'context_length': 32, 'batch_size': 32, 'max_epochs': 30, 'learning_rate': 0.00034593338048612984, 'aug_prob': 0.2769782795584747}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.26417 (best 0.26417), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.26417 (best 0.26417), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.06888 (best 0.06888), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.06888 (best 0.06888), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.28283 (best -0.28283), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.28283 (best -0.28283), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.54135 (best -0.54135), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.54135 (best -0.54135), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -0.80239 (best -0.80239), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -0.80239 (best -0.80239), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -0.85514 (best -0.85514), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -0.85514 (best -0.85514), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -0.97759 (best -0.97759), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -0.97759 (best -0.97759), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.11291 (best -1.11291), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.11291 (best -1.11291), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.11895 (best -1.11895), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.11895 (best -1.11895), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.23167 (best -1.23167), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.23167 (best -1.23167), saving model to '/content/lag-llama/lightning_logs/version_28/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 28: MAE=2.29, RMSE=2.52\n",
            "[I 2025-11-24 02:56:33,838] Trial 28 finished with value: 2.289926528930664 and parameters: {'context_length': 16, 'batch_size': 8, 'max_epochs': 20, 'learning_rate': 9.350042450036866e-05, 'aug_prob': 0.20412063284889093}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.21976 (best 0.21976), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.21976 (best 0.21976), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.24607 (best -0.24607), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.24607 (best -0.24607), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.57868 (best -0.57868), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.57868 (best -0.57868), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.60161 (best -0.60161), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.60161 (best -0.60161), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.81682 (best -0.81682), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.81682 (best -0.81682), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.98354 (best -0.98354), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.98354 (best -0.98354), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.99539 (best -0.99539), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.99539 (best -0.99539), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.27513 (best -1.27513), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.27513 (best -1.27513), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.43270 (best -1.43270), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.43270 (best -1.43270), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -1.47332 (best -1.47332), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -1.47332 (best -1.47332), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.58006 (best -1.58006), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.58006 (best -1.58006), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.59283 (best -1.59283), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.59283 (best -1.59283), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' reached -1.88446 (best -1.88446), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' reached -1.88446 (best -1.88446), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' reached -1.98968 (best -1.98968), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' reached -1.98968 (best -1.98968), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' reached -2.02751 (best -2.02751), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' reached -2.02751 (best -2.02751), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' reached -2.21054 (best -2.21054), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=36-step=1850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' reached -2.21054 (best -2.21054), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=36-step=1850.ckpt' as top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' reached -2.27799 (best -2.27799), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=40-step=2050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' reached -2.27799 (best -2.27799), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=40-step=2050.ckpt' as top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' reached -2.30988 (best -2.30988), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=47-step=2400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' reached -2.30988 (best -2.30988), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=47-step=2400.ckpt' as top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' reached -2.35205 (best -2.35205), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=56-step=2850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' reached -2.35205 (best -2.35205), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=56-step=2850.ckpt' as top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' reached -2.36542 (best -2.36542), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=68-step=3450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' reached -2.36542 (best -2.36542), saving model to '/content/lag-llama/lightning_logs/version_29/checkpoints/epoch=68-step=3450.ckpt' as top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=70` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=70` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 29: MAE=2.26, RMSE=2.62\n",
            "[I 2025-11-24 02:58:51,011] Trial 29 finished with value: 2.2564570903778076 and parameters: {'context_length': 32, 'batch_size': 16, 'max_epochs': 70, 'learning_rate': 4.559651392878715e-05, 'aug_prob': 0.16783254258746094}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.17526 (best 0.17526), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.17526 (best 0.17526), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.15202 (best -0.15202), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.15202 (best -0.15202), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.43768 (best -0.43768), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.43768 (best -0.43768), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.62834 (best -0.62834), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.62834 (best -0.62834), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.73236 (best -0.73236), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.73236 (best -0.73236), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.90449 (best -0.90449), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.90449 (best -0.90449), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.10781 (best -1.10781), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.10781 (best -1.10781), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.13483 (best -1.13483), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.13483 (best -1.13483), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.22766 (best -1.22766), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.22766 (best -1.22766), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.25452 (best -1.25452), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.25452 (best -1.25452), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.52292 (best -1.52292), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.52292 (best -1.52292), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.61248 (best -1.61248), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.61248 (best -1.61248), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.72603 (best -1.72603), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.72603 (best -1.72603), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' reached -1.74485 (best -1.74485), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' reached -1.74485 (best -1.74485), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' reached -1.79731 (best -1.79731), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=28-step=1450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' reached -1.79731 (best -1.79731), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=28-step=1450.ckpt' as top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' reached -1.97634 (best -1.97634), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' reached -1.97634 (best -1.97634), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' reached -2.07295 (best -2.07295), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' reached -2.07295 (best -2.07295), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' reached -2.25879 (best -2.25879), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=46-step=2350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' reached -2.25879 (best -2.25879), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=46-step=2350.ckpt' as top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' reached -2.33650 (best -2.33650), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=49-step=2500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' reached -2.33650 (best -2.33650), saving model to '/content/lag-llama/lightning_logs/version_30/checkpoints/epoch=49-step=2500.ckpt' as top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30: MAE=1.71, RMSE=2.02\n",
            "[I 2025-11-24 03:00:26,544] Trial 30 finished with value: 1.709997296333313 and parameters: {'context_length': 64, 'batch_size': 8, 'max_epochs': 50, 'learning_rate': 2.4640044520707184e-05, 'aug_prob': 0.22831239091009523}. Best is trial 15 with value: 1.42127525806427.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.15953 (best 0.15953), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.15953 (best 0.15953), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.20814 (best -0.20814), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.20814 (best -0.20814), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.59250 (best -0.59250), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.59250 (best -0.59250), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.79868 (best -0.79868), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.79868 (best -0.79868), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.95469 (best -0.95469), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.95469 (best -0.95469), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.99360 (best -0.99360), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.99360 (best -0.99360), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.18040 (best -1.18040), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.18040 (best -1.18040), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.18309 (best -1.18309), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.18309 (best -1.18309), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.31164 (best -1.31164), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.31164 (best -1.31164), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.38467 (best -1.38467), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.38467 (best -1.38467), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.62384 (best -1.62384), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.62384 (best -1.62384), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.72377 (best -1.72377), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.72377 (best -1.72377), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.76775 (best -1.76775), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.76775 (best -1.76775), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -1.92183 (best -1.92183), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -1.92183 (best -1.92183), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' reached -2.18414 (best -2.18414), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' reached -2.18414 (best -2.18414), saving model to '/content/lag-llama/lightning_logs/version_31/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 31: MAE=1.40, RMSE=1.83\n",
            "[I 2025-11-24 03:01:52,978] Trial 31 finished with value: 1.4004262685775757 and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 40, 'learning_rate': 2.9457407395652907e-05, 'aug_prob': 0.2475333588532187}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.24974 (best 0.24974), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.24974 (best 0.24974), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.06088 (best -0.06088), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.06088 (best -0.06088), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.29958 (best -0.29958), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.29958 (best -0.29958), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.57094 (best -0.57094), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.57094 (best -0.57094), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.69598 (best -0.69598), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.69598 (best -0.69598), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.85803 (best -0.85803), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.85803 (best -0.85803), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.08494 (best -1.08494), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.08494 (best -1.08494), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.17161 (best -1.17161), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.17161 (best -1.17161), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.19729 (best -1.19729), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.19729 (best -1.19729), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.28067 (best -1.28067), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.28067 (best -1.28067), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.29016 (best -1.29016), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.29016 (best -1.29016), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.29832 (best -1.29832), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.29832 (best -1.29832), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.40046 (best -1.40046), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.40046 (best -1.40046), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.42021 (best -1.42021), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.42021 (best -1.42021), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.55716 (best -1.55716), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.55716 (best -1.55716), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.68013 (best -1.68013), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.68013 (best -1.68013), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -1.73159 (best -1.73159), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -1.73159 (best -1.73159), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' reached -1.79823 (best -1.79823), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' reached -1.79823 (best -1.79823), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' reached -2.17160 (best -2.17160), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' reached -2.17160 (best -2.17160), saving model to '/content/lag-llama/lightning_logs/version_32/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 32 failed: \n",
            "[I 2025-11-24 03:03:21,077] Trial 32 finished with value: inf and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 40, 'learning_rate': 1.6266292542052435e-05, 'aug_prob': 0.2797220751376096}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.13961 (best 0.13961), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.13961 (best 0.13961), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.32844 (best -0.32844), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.32844 (best -0.32844), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.62327 (best -0.62327), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.62327 (best -0.62327), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.89902 (best -0.89902), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.89902 (best -0.89902), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -1.20007 (best -1.20007), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -1.20007 (best -1.20007), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.49141 (best -1.49141), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.49141 (best -1.49141), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.50654 (best -1.50654), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.50654 (best -1.50654), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.52045 (best -1.52045), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.52045 (best -1.52045), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.55738 (best -1.55738), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.55738 (best -1.55738), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.76917 (best -1.76917), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.76917 (best -1.76917), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.82172 (best -1.82172), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.82172 (best -1.82172), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -2.10221 (best -2.10221), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -2.10221 (best -2.10221), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' reached -2.22210 (best -2.22210), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' reached -2.22210 (best -2.22210), saving model to '/content/lag-llama/lightning_logs/version_33/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 33: MAE=1.62, RMSE=1.99\n",
            "[I 2025-11-24 03:04:26,489] Trial 33 finished with value: 1.6186734437942505 and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 30, 'learning_rate': 3.4704346685202564e-05, 'aug_prob': 0.21425330748910368}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.06496 (best 0.06496), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.06496 (best 0.06496), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.47117 (best -0.47117), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.47117 (best -0.47117), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.90418 (best -0.90418), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.90418 (best -0.90418), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -1.14343 (best -1.14343), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -1.14343 (best -1.14343), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.37502 (best -1.37502), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.37502 (best -1.37502), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.73748 (best -1.73748), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.73748 (best -1.73748), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.75757 (best -1.75757), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.75757 (best -1.75757), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -2.01531 (best -2.01531), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -2.01531 (best -2.01531), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -2.09043 (best -2.09043), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -2.09043 (best -2.09043), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' reached -2.10582 (best -2.10582), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=39-step=2000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' reached -2.10582 (best -2.10582), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=39-step=2000.ckpt' as top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' reached -2.26625 (best -2.26625), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=46-step=2350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' reached -2.26625 (best -2.26625), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=46-step=2350.ckpt' as top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' reached -2.29086 (best -2.29086), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=47-step=2400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' reached -2.29086 (best -2.29086), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=47-step=2400.ckpt' as top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' reached -2.53843 (best -2.53843), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=49-step=2500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' reached -2.53843 (best -2.53843), saving model to '/content/lag-llama/lightning_logs/version_34/checkpoints/epoch=49-step=2500.ckpt' as top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=70` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=70` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 34 failed: \n",
            "[I 2025-11-24 03:06:57,115] Trial 34 finished with value: inf and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 70, 'learning_rate': 6.748131129379857e-05, 'aug_prob': 0.24220147980303344}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.23073 (best 0.23073), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.23073 (best 0.23073), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.39655 (best -0.39655), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.39655 (best -0.39655), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.58257 (best -0.58257), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.58257 (best -0.58257), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.70074 (best -0.70074), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.70074 (best -0.70074), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.97142 (best -0.97142), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.97142 (best -0.97142), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.23421 (best -1.23421), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.23421 (best -1.23421), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.48156 (best -1.48156), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.48156 (best -1.48156), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.53436 (best -1.53436), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.53436 (best -1.53436), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.57926 (best -1.57926), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.57926 (best -1.57926), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.59315 (best -1.59315), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.59315 (best -1.59315), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.77493 (best -1.77493), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.77493 (best -1.77493), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.94855 (best -1.94855), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.94855 (best -1.94855), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' reached -2.04120 (best -2.04120), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' reached -2.04120 (best -2.04120), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' reached -2.05209 (best -2.05209), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' reached -2.05209 (best -2.05209), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' reached -2.18055 (best -2.18055), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=28-step=1450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' reached -2.18055 (best -2.18055), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=28-step=1450.ckpt' as top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' reached -2.18786 (best -2.18786), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=37-step=1900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' reached -2.18786 (best -2.18786), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=37-step=1900.ckpt' as top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' reached -2.21128 (best -2.21128), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=39-step=2000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' reached -2.21128 (best -2.21128), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=39-step=2000.ckpt' as top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' reached -2.33538 (best -2.33538), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=48-step=2450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' reached -2.33538 (best -2.33538), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=48-step=2450.ckpt' as top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' reached -2.36963 (best -2.36963), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=51-step=2600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' reached -2.36963 (best -2.36963), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=51-step=2600.ckpt' as top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' reached -2.52133 (best -2.52133), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=52-step=2650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' reached -2.52133 (best -2.52133), saving model to '/content/lag-llama/lightning_logs/version_35/checkpoints/epoch=52-step=2650.ckpt' as top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 35: MAE=2.68, RMSE=3.02\n",
            "[I 2025-11-24 03:09:06,945] Trial 35 finished with value: 2.6828320026397705 and parameters: {'context_length': 32, 'batch_size': 32, 'max_epochs': 60, 'learning_rate': 4.21803495936163e-05, 'aug_prob': 0.11833492927168998}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.04435 (best 0.04435), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.04435 (best 0.04435), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.55036 (best -0.55036), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.55036 (best -0.55036), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.76389 (best -0.76389), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.76389 (best -0.76389), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -1.09901 (best -1.09901), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -1.09901 (best -1.09901), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -1.28090 (best -1.28090), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -1.28090 (best -1.28090), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.37787 (best -1.37787), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.37787 (best -1.37787), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.50112 (best -1.50112), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.50112 (best -1.50112), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.59178 (best -1.59178), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.59178 (best -1.59178), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.73075 (best -1.73075), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.73075 (best -1.73075), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -1.77172 (best -1.77172), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -1.77172 (best -1.77172), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' reached -1.98440 (best -1.98440), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' reached -1.98440 (best -1.98440), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' reached -1.98650 (best -1.98650), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' reached -1.98650 (best -1.98650), saving model to '/content/lag-llama/lightning_logs/version_36/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 36 failed: \n",
            "[I 2025-11-24 03:10:27,066] Trial 36 finished with value: inf and parameters: {'context_length': 96, 'batch_size': 16, 'max_epochs': 40, 'learning_rate': 0.00021997268784713613, 'aug_prob': 0.1581149089599624}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.38339 (best 0.38339), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.38339 (best 0.38339), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.14024 (best 0.14024), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.14024 (best 0.14024), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.09080 (best -0.09080), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.09080 (best -0.09080), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.26139 (best -0.26139), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.26139 (best -0.26139), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.47771 (best -0.47771), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.47771 (best -0.47771), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.55451 (best -0.55451), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.55451 (best -0.55451), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -0.68431 (best -0.68431), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -0.68431 (best -0.68431), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -0.86146 (best -0.86146), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -0.86146 (best -0.86146), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.05222 (best -1.05222), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.05222 (best -1.05222), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.10115 (best -1.10115), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.10115 (best -1.10115), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.12934 (best -1.12934), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.12934 (best -1.12934), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.13632 (best -1.13632), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.13632 (best -1.13632), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -1.26153 (best -1.26153), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -1.26153 (best -1.26153), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' reached -1.34795 (best -1.34795), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' reached -1.34795 (best -1.34795), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' reached -1.43172 (best -1.43172), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' reached -1.43172 (best -1.43172), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' reached -1.59256 (best -1.59256), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=38-step=1950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' reached -1.59256 (best -1.59256), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=38-step=1950.ckpt' as top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' reached -1.74497 (best -1.74497), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=54-step=2750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' reached -1.74497 (best -1.74497), saving model to '/content/lag-llama/lightning_logs/version_37/checkpoints/epoch=54-step=2750.ckpt' as top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 37: MAE=1.45, RMSE=1.89\n",
            "[I 2025-11-24 03:12:36,127] Trial 37 finished with value: 1.4467978477478027 and parameters: {'context_length': 16, 'batch_size': 32, 'max_epochs': 60, 'learning_rate': 1.836688933090829e-05, 'aug_prob': 0.2727361204113515}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.49509 (best 0.49509), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.49509 (best 0.49509), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.11299 (best 0.11299), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.11299 (best 0.11299), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.15802 (best -0.15802), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.15802 (best -0.15802), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.23839 (best -0.23839), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.23839 (best -0.23839), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.35933 (best -0.35933), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.35933 (best -0.35933), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.35965 (best -0.35965), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.35965 (best -0.35965), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.53914 (best -0.53914), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.53914 (best -0.53914), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -0.71532 (best -0.71532), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -0.71532 (best -0.71532), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -0.77831 (best -0.77831), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -0.77831 (best -0.77831), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -0.78327 (best -0.78327), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -0.78327 (best -0.78327), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -0.90512 (best -0.90512), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -0.90512 (best -0.90512), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -0.95432 (best -0.95432), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -0.95432 (best -0.95432), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.02124 (best -1.02124), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.02124 (best -1.02124), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' reached -1.21701 (best -1.21701), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' reached -1.21701 (best -1.21701), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' reached -1.23395 (best -1.23395), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' reached -1.23395 (best -1.23395), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' reached -1.42409 (best -1.42409), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=31-step=1600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' reached -1.42409 (best -1.42409), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=31-step=1600.ckpt' as top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' reached -1.48883 (best -1.48883), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' reached -1.48883 (best -1.48883), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' reached -1.49149 (best -1.49149), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=39-step=2000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' reached -1.49149 (best -1.49149), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=39-step=2000.ckpt' as top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' reached -1.61196 (best -1.61196), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=47-step=2400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' reached -1.61196 (best -1.61196), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=47-step=2400.ckpt' as top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' reached -1.64071 (best -1.64071), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=53-step=2700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' reached -1.64071 (best -1.64071), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=53-step=2700.ckpt' as top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' reached -1.68328 (best -1.68328), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=55-step=2800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' reached -1.68328 (best -1.68328), saving model to '/content/lag-llama/lightning_logs/version_38/checkpoints/epoch=55-step=2800.ckpt' as top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 38: MAE=1.75, RMSE=2.18\n",
            "[I 2025-11-24 03:14:34,234] Trial 38 finished with value: 1.7519057989120483 and parameters: {'context_length': 16, 'batch_size': 16, 'max_epochs': 60, 'learning_rate': 1.5239566491074467e-05, 'aug_prob': 0.204562206934738}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.46883 (best 0.46883), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.46883 (best 0.46883), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.19327 (best 0.19327), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.19327 (best 0.19327), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached 0.00578 (best 0.00578), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached 0.00578 (best 0.00578), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.15728 (best -0.15728), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.15728 (best -0.15728), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.22026 (best -0.22026), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.22026 (best -0.22026), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.30711 (best -0.30711), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.30711 (best -0.30711), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.51060 (best -0.51060), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.51060 (best -0.51060), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -0.54689 (best -0.54689), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -0.54689 (best -0.54689), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -0.59040 (best -0.59040), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -0.59040 (best -0.59040), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -0.65944 (best -0.65944), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -0.65944 (best -0.65944), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -0.92642 (best -0.92642), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -0.92642 (best -0.92642), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -0.97600 (best -0.97600), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -0.97600 (best -0.97600), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' reached -1.21497 (best -1.21497), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' reached -1.21497 (best -1.21497), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' reached -1.22065 (best -1.22065), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' reached -1.22065 (best -1.22065), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' reached -1.34203 (best -1.34203), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=39-step=2000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' reached -1.34203 (best -1.34203), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=39-step=2000.ckpt' as top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' reached -1.35871 (best -1.35871), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=40-step=2050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' reached -1.35871 (best -1.35871), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=40-step=2050.ckpt' as top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' reached -1.57133 (best -1.57133), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=42-step=2150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' reached -1.57133 (best -1.57133), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=42-step=2150.ckpt' as top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' reached -1.65777 (best -1.65777), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=51-step=2600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' reached -1.65777 (best -1.65777), saving model to '/content/lag-llama/lightning_logs/version_39/checkpoints/epoch=51-step=2600.ckpt' as top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=60` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 39: MAE=1.77, RMSE=2.20\n",
            "[I 2025-11-24 03:16:43,905] Trial 39 finished with value: 1.7728430032730103 and parameters: {'context_length': 16, 'batch_size': 32, 'max_epochs': 60, 'learning_rate': 1.2679617423145258e-05, 'aug_prob': 0.2872750662404558}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.29987 (best 0.29987), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.29987 (best 0.29987), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.19455 (best -0.19455), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.19455 (best -0.19455), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.45616 (best -0.45616), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.45616 (best -0.45616), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.47959 (best -0.47959), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.47959 (best -0.47959), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.53182 (best -0.53182), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.53182 (best -0.53182), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.61980 (best -0.61980), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.61980 (best -0.61980), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.03240 (best -1.03240), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.03240 (best -1.03240), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.18108 (best -1.18108), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.18108 (best -1.18108), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.18776 (best -1.18776), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.18776 (best -1.18776), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -1.33696 (best -1.33696), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -1.33696 (best -1.33696), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.59128 (best -1.59128), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.59128 (best -1.59128), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' reached -1.78055 (best -1.78055), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' reached -1.78055 (best -1.78055), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' reached -1.79114 (best -1.79114), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' reached -1.79114 (best -1.79114), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' reached -1.86511 (best -1.86511), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' reached -1.86511 (best -1.86511), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' reached -1.93482 (best -1.93482), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=43-step=2200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' reached -1.93482 (best -1.93482), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=43-step=2200.ckpt' as top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' reached -2.04173 (best -2.04173), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=46-step=2350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' reached -2.04173 (best -2.04173), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=46-step=2350.ckpt' as top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 72, global step 3650: 'train_loss' reached -2.16360 (best -2.16360), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=72-step=3650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 72, global step 3650: 'train_loss' reached -2.16360 (best -2.16360), saving model to '/content/lag-llama/lightning_logs/version_40/checkpoints/epoch=72-step=3650.ckpt' as top 1\n",
            "INFO: Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=80` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=80` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 40 failed: \n",
            "[I 2025-11-24 03:19:10,742] Trial 40 finished with value: inf and parameters: {'context_length': 16, 'batch_size': 8, 'max_epochs': 80, 'learning_rate': 0.00012514633338303933, 'aug_prob': 0.07062554556640983}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.43257 (best 0.43257), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.43257 (best 0.43257), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.10527 (best 0.10527), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.10527 (best 0.10527), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.10299 (best -0.10299), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.10299 (best -0.10299), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.28449 (best -0.28449), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.28449 (best -0.28449), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.36121 (best -0.36121), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.36121 (best -0.36121), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.45644 (best -0.45644), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.45644 (best -0.45644), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.75633 (best -0.75633), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.75633 (best -0.75633), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -0.79165 (best -0.79165), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -0.79165 (best -0.79165), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.08604 (best -1.08604), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.08604 (best -1.08604), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -1.09334 (best -1.09334), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -1.09334 (best -1.09334), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.12334 (best -1.12334), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.12334 (best -1.12334), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.18734 (best -1.18734), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.18734 (best -1.18734), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.38518 (best -1.38518), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.38518 (best -1.38518), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' reached -1.41999 (best -1.41999), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' reached -1.41999 (best -1.41999), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' reached -1.60441 (best -1.60441), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=41-step=2100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' reached -1.60441 (best -1.60441), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=41-step=2100.ckpt' as top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' reached -1.74415 (best -1.74415), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=44-step=2250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' reached -1.74415 (best -1.74415), saving model to '/content/lag-llama/lightning_logs/version_41/checkpoints/epoch=44-step=2250.ckpt' as top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 41: MAE=1.52, RMSE=1.96\n",
            "[I 2025-11-24 03:20:59,611] Trial 41 finished with value: 1.5207023620605469 and parameters: {'context_length': 16, 'batch_size': 32, 'max_epochs': 50, 'learning_rate': 2.0482364736666262e-05, 'aug_prob': 0.2678429006763013}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.36664 (best 0.36664), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.36664 (best 0.36664), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.11180 (best 0.11180), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.11180 (best 0.11180), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.10726 (best -0.10726), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.10726 (best -0.10726), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.47651 (best -0.47651), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.47651 (best -0.47651), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.53851 (best -0.53851), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.53851 (best -0.53851), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.61989 (best -0.61989), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.61989 (best -0.61989), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.63373 (best -0.63373), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.63373 (best -0.63373), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -0.82461 (best -0.82461), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -0.82461 (best -0.82461), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -0.85780 (best -0.85780), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -0.85780 (best -0.85780), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -0.90140 (best -0.90140), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -0.90140 (best -0.90140), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -0.92768 (best -0.92768), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -0.92768 (best -0.92768), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -0.96177 (best -0.96177), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -0.96177 (best -0.96177), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -0.97987 (best -0.97987), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -0.97987 (best -0.97987), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.07722 (best -1.07722), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.07722 (best -1.07722), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.10521 (best -1.10521), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.10521 (best -1.10521), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' reached -1.22877 (best -1.22877), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' reached -1.22877 (best -1.22877), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' reached -1.23534 (best -1.23534), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' reached -1.23534 (best -1.23534), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' reached -1.27327 (best -1.27327), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' reached -1.27327 (best -1.27327), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' reached -1.29266 (best -1.29266), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' reached -1.29266 (best -1.29266), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' reached -1.30298 (best -1.30298), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' reached -1.30298 (best -1.30298), saving model to '/content/lag-llama/lightning_logs/version_42/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 42 failed: \n",
            "[I 2025-11-24 03:22:05,337] Trial 42 finished with value: inf and parameters: {'context_length': 16, 'batch_size': 32, 'max_epochs': 30, 'learning_rate': 2.483967284402461e-05, 'aug_prob': 0.25772702011811893}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.04826 (best 0.04826), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.04826 (best 0.04826), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.18352 (best -0.18352), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.18352 (best -0.18352), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.78574 (best -0.78574), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.78574 (best -0.78574), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.80522 (best -0.80522), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.80522 (best -0.80522), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -1.10294 (best -1.10294), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -1.10294 (best -1.10294), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -1.11908 (best -1.11908), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -1.11908 (best -1.11908), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.19183 (best -1.19183), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.19183 (best -1.19183), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -1.31805 (best -1.31805), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -1.31805 (best -1.31805), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.59823 (best -1.59823), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.59823 (best -1.59823), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -1.64644 (best -1.64644), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -1.64644 (best -1.64644), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.71278 (best -1.71278), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.71278 (best -1.71278), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.93054 (best -1.93054), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.93054 (best -1.93054), saving model to '/content/lag-llama/lightning_logs/version_43/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 43: MAE=1.63, RMSE=2.00\n",
            "[I 2025-11-24 03:22:58,300] Trial 43 finished with value: 1.6277918815612793 and parameters: {'context_length': 96, 'batch_size': 32, 'max_epochs': 20, 'learning_rate': 2.9448611319597265e-05, 'aug_prob': 0.23564633819345737}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.25601 (best 0.25601), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.25601 (best 0.25601), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.01754 (best 0.01754), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.01754 (best 0.01754), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.23266 (best -0.23266), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.23266 (best -0.23266), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.40717 (best -0.40717), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.40717 (best -0.40717), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.50847 (best -0.50847), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.50847 (best -0.50847), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.75478 (best -0.75478), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.75478 (best -0.75478), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.90539 (best -0.90539), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.90539 (best -0.90539), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -0.95508 (best -0.95508), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -0.95508 (best -0.95508), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.41764 (best -1.41764), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.41764 (best -1.41764), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -1.51496 (best -1.51496), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -1.51496 (best -1.51496), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' reached -1.68197 (best -1.68197), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' reached -1.68197 (best -1.68197), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' reached -1.80441 (best -1.80441), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' reached -1.80441 (best -1.80441), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' reached -1.96259 (best -1.96259), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' reached -1.96259 (best -1.96259), saving model to '/content/lag-llama/lightning_logs/version_44/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=40` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 44 failed: \n",
            "[I 2025-11-24 03:24:25,744] Trial 44 finished with value: inf and parameters: {'context_length': 64, 'batch_size': 32, 'max_epochs': 40, 'learning_rate': 1.2759197948402405e-05, 'aug_prob': 0.2746359163430955}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.26550 (best 0.26550), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.26550 (best 0.26550), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.00718 (best 0.00718), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.00718 (best 0.00718), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.28563 (best -0.28563), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.28563 (best -0.28563), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.39400 (best -0.39400), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.39400 (best -0.39400), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.62281 (best -0.62281), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.62281 (best -0.62281), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.69940 (best -0.69940), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.69940 (best -0.69940), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.89856 (best -0.89856), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.89856 (best -0.89856), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' reached -0.99284 (best -0.99284), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' reached -0.99284 (best -0.99284), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.12688 (best -1.12688), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.12688 (best -1.12688), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.15489 (best -1.15489), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.15489 (best -1.15489), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.30766 (best -1.30766), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.30766 (best -1.30766), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.35713 (best -1.35713), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.35713 (best -1.35713), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -1.52032 (best -1.52032), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -1.52032 (best -1.52032), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.54029 (best -1.54029), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.54029 (best -1.54029), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.70024 (best -1.70024), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.70024 (best -1.70024), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' reached -2.11880 (best -2.11880), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' reached -2.11880 (best -2.11880), saving model to '/content/lag-llama/lightning_logs/version_45/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 45: MAE=1.59, RMSE=1.95\n",
            "[I 2025-11-24 03:26:06,838] Trial 45 finished with value: 1.5949195623397827 and parameters: {'context_length': 64, 'batch_size': 16, 'max_epochs': 50, 'learning_rate': 1.839752261029923e-05, 'aug_prob': 0.2992029684070697}. Best is trial 31 with value: 1.4004262685775757.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.33332 (best 0.33332), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.33332 (best 0.33332), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.05014 (best -0.05014), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.05014 (best -0.05014), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.40961 (best -0.40961), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.40961 (best -0.40961), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.66032 (best -0.66032), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.66032 (best -0.66032), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.92822 (best -0.92822), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.92822 (best -0.92822), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.14478 (best -1.14478), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.14478 (best -1.14478), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.16405 (best -1.16405), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.16405 (best -1.16405), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.19833 (best -1.19833), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.19833 (best -1.19833), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.31659 (best -1.31659), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.31659 (best -1.31659), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' reached -1.33848 (best -1.33848), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' reached -1.33848 (best -1.33848), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.34021 (best -1.34021), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.34021 (best -1.34021), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' reached -1.49785 (best -1.49785), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' reached -1.49785 (best -1.49785), saving model to '/content/lag-llama/lightning_logs/version_46/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 46: MAE=1.38, RMSE=1.82\n",
            "[I 2025-11-24 03:27:12,855] Trial 46 finished with value: 1.3824435472488403 and parameters: {'context_length': 16, 'batch_size': 32, 'max_epochs': 30, 'learning_rate': 3.562575680498601e-05, 'aug_prob': 0.2469149578417666}. Best is trial 46 with value: 1.3824435472488403.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.34010 (best 0.34010), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.34010 (best 0.34010), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.21698 (best -0.21698), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.21698 (best -0.21698), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.31871 (best -0.31871), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.31871 (best -0.31871), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.49183 (best -0.49183), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.49183 (best -0.49183), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.49687 (best -0.49687), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.49687 (best -0.49687), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.69642 (best -0.69642), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.69642 (best -0.69642), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' reached -0.87333 (best -0.87333), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' reached -0.87333 (best -0.87333), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -1.03302 (best -1.03302), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -1.03302 (best -1.03302), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -1.03439 (best -1.03439), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -1.03439 (best -1.03439), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.31386 (best -1.31386), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.31386 (best -1.31386), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' reached -1.36282 (best -1.36282), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' reached -1.36282 (best -1.36282), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' reached -1.57254 (best -1.57254), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' reached -1.57254 (best -1.57254), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' reached -1.95104 (best -1.95104), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=38-step=1950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' reached -1.95104 (best -1.95104), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=38-step=1950.ckpt' as top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' reached -2.01450 (best -2.01450), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=58-step=2950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' reached -2.01450 (best -2.01450), saving model to '/content/lag-llama/lightning_logs/version_47/checkpoints/epoch=58-step=2950.ckpt' as top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=70` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=70` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 47 failed: \n",
            "[I 2025-11-24 03:29:43,071] Trial 47 finished with value: inf and parameters: {'context_length': 16, 'batch_size': 32, 'max_epochs': 70, 'learning_rate': 3.645022956510178e-05, 'aug_prob': 0.2514315456847009}. Best is trial 46 with value: 1.3824435472488403.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.24750 (best 0.24750), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.24750 (best 0.24750), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.15581 (best -0.15581), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.15581 (best -0.15581), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.30544 (best -0.30544), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.30544 (best -0.30544), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.65361 (best -0.65361), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.65361 (best -0.65361), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.75489 (best -0.75489), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.75489 (best -0.75489), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -0.82443 (best -0.82443), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -0.82443 (best -0.82443), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -0.99514 (best -0.99514), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -0.99514 (best -0.99514), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -1.05848 (best -1.05848), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -1.05848 (best -1.05848), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.10702 (best -1.10702), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.10702 (best -1.10702), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' reached -1.25196 (best -1.25196), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' reached -1.25196 (best -1.25196), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' reached -1.28165 (best -1.28165), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' reached -1.28165 (best -1.28165), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.31197 (best -1.31197), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.31197 (best -1.31197), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -1.45689 (best -1.45689), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -1.45689 (best -1.45689), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' reached -1.56607 (best -1.56607), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=37-step=1900.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' reached -1.56607 (best -1.56607), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=37-step=1900.ckpt' as top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' reached -1.57787 (best -1.57787), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=40-step=2050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' reached -1.57787 (best -1.57787), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=40-step=2050.ckpt' as top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' reached -1.57827 (best -1.57827), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=47-step=2400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' reached -1.57827 (best -1.57827), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=47-step=2400.ckpt' as top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' reached -1.67967 (best -1.67967), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=48-step=2450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' reached -1.67967 (best -1.67967), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=48-step=2450.ckpt' as top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' reached -1.72921 (best -1.72921), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=62-step=3150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' reached -1.72921 (best -1.72921), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=62-step=3150.ckpt' as top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 78, global step 3950: 'train_loss' reached -1.99669 (best -1.99669), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=78-step=3950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 78, global step 3950: 'train_loss' reached -1.99669 (best -1.99669), saving model to '/content/lag-llama/lightning_logs/version_48/checkpoints/epoch=78-step=3950.ckpt' as top 1\n",
            "INFO: Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=80` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=80` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 48 failed: \n",
            "[I 2025-11-24 03:32:21,399] Trial 48 finished with value: inf and parameters: {'context_length': 16, 'batch_size': 16, 'max_epochs': 80, 'learning_rate': 5.266235319530375e-05, 'aug_prob': 0.2860721110245605}. Best is trial 46 with value: 1.3824435472488403.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.15055 (best 0.15055), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.15055 (best 0.15055), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.34919 (best -0.34919), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.34919 (best -0.34919), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.46736 (best -0.46736), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.46736 (best -0.46736), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.53544 (best -0.53544), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.53544 (best -0.53544), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.73378 (best -0.73378), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.73378 (best -0.73378), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.74684 (best -0.74684), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.74684 (best -0.74684), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -1.24737 (best -1.24737), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -1.24737 (best -1.24737), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -1.37474 (best -1.37474), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -1.37474 (best -1.37474), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -1.46320 (best -1.46320), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -1.46320 (best -1.46320), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' reached -1.56292 (best -1.56292), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' reached -1.56292 (best -1.56292), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' reached -1.65576 (best -1.65576), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' reached -1.65576 (best -1.65576), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' reached -1.72281 (best -1.72281), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' reached -1.72281 (best -1.72281), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' reached -1.90237 (best -1.90237), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' reached -1.90237 (best -1.90237), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' reached -1.96016 (best -1.96016), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=51-step=2600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' reached -1.96016 (best -1.96016), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=51-step=2600.ckpt' as top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' reached -1.97048 (best -1.97048), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=60-step=3050.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' reached -1.97048 (best -1.97048), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=60-step=3050.ckpt' as top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' reached -2.10286 (best -2.10286), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=65-step=3300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' reached -2.10286 (best -2.10286), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=65-step=3300.ckpt' as top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' reached -2.18026 (best -2.18026), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=69-step=3500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' reached -2.18026 (best -2.18026), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=69-step=3500.ckpt' as top 1\n",
            "INFO: Epoch 70, global step 3550: 'train_loss' reached -2.20484 (best -2.20484), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=70-step=3550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 70, global step 3550: 'train_loss' reached -2.20484 (best -2.20484), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=70-step=3550.ckpt' as top 1\n",
            "INFO: Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 72, global step 3650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 80, global step 4050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 80, global step 4050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 81, global step 4100: 'train_loss' reached -2.23898 (best -2.23898), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=81-step=4100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 81, global step 4100: 'train_loss' reached -2.23898 (best -2.23898), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=81-step=4100.ckpt' as top 1\n",
            "INFO: Epoch 82, global step 4150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 82, global step 4150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 83, global step 4200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 83, global step 4200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 84, global step 4250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 84, global step 4250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 85, global step 4300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 85, global step 4300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 86, global step 4350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 86, global step 4350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 87, global step 4400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 87, global step 4400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 88, global step 4450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 88, global step 4450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 89, global step 4500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 89, global step 4500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 90, global step 4550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 90, global step 4550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 91, global step 4600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 91, global step 4600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 92, global step 4650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 92, global step 4650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 93, global step 4700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 93, global step 4700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 94, global step 4750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 94, global step 4750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 95, global step 4800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 95, global step 4800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 96, global step 4850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 96, global step 4850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 97, global step 4900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 97, global step 4900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 98, global step 4950: 'train_loss' reached -2.36832 (best -2.36832), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=98-step=4950.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 98, global step 4950: 'train_loss' reached -2.36832 (best -2.36832), saving model to '/content/lag-llama/lightning_logs/version_49/checkpoints/epoch=98-step=4950.ckpt' as top 1\n",
            "INFO: Epoch 99, global step 5000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 99, global step 5000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 49 failed: \n",
            "[I 2025-11-24 03:35:55,309] Trial 49 finished with value: inf and parameters: {'context_length': 16, 'batch_size': 32, 'max_epochs': 100, 'learning_rate': 8.784722547400572e-05, 'aug_prob': 0.2164147393757509}. Best is trial 46 with value: 1.3824435472488403.\n",
            "\n",
            "======================================================================\n",
            "✅ OPTIMIZATION COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "🏆 BEST HYPERPARAMETERS:\n",
            "--------------------------------------------------\n",
            "  • context_length: 16\n",
            "  • batch_size: 32\n",
            "  • max_epochs: 30\n",
            "  • learning_rate: 3.562575680498601e-05\n",
            "  • aug_prob: 0.2469149578417666\n",
            "\n",
            "📊 BEST VALIDATION MAE: 1.38\n",
            "📈 Total trials completed: 50\n",
            "✓ Completed: 50\n",
            "✗ Pruned: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "🎯 TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2658e67458f54305892976a4abaec7d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 0.52506 (best 0.52506), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 0.52506 (best 0.52506), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached -0.07192 (best -0.07192), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached -0.07192 (best -0.07192), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached -0.24596 (best -0.24596), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached -0.24596 (best -0.24596), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' reached -0.25713 (best -0.25713), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' reached -0.25713 (best -0.25713), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached -0.55729 (best -0.55729), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached -0.55729 (best -0.55729), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' reached -0.73318 (best -0.73318), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' reached -0.73318 (best -0.73318), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' reached -0.81950 (best -0.81950), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' reached -0.81950 (best -0.81950), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' reached -0.82720 (best -0.82720), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' reached -0.82720 (best -0.82720), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' reached -0.83355 (best -0.83355), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' reached -0.83355 (best -0.83355), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' reached -0.83612 (best -0.83612), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' reached -0.83612 (best -0.83612), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' reached -1.03047 (best -1.03047), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' reached -1.03047 (best -1.03047), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' reached -1.21727 (best -1.21727), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' reached -1.21727 (best -1.21727), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -1.33299 (best -1.33299), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -1.33299 (best -1.33299), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' reached -1.47937 (best -1.47937), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' reached -1.47937 (best -1.47937), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' reached -1.59923 (best -1.59923), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' reached -1.59923 (best -1.59923), saving model to '/content/lag-llama/lightning_logs/version_50/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "✅ FINAL MODEL EVALUATION (TEST SET)\n",
            "======================================================================\n",
            "\n",
            "📊 ACCURACY METRICS:\n",
            "  • MAE (Mean Absolute Error):        3.57\n",
            "  • RMSE (Root Mean Squared Error):   4.01\n",
            "  • MAPE (Mean Absolute % Error):     20.1%\n",
            "\n",
            "🏆 BEST HYPERPARAMETERS USED:\n",
            "  • context_length: 16\n",
            "  • batch_size: 32\n",
            "  • max_epochs: 30\n",
            "  • learning_rate: 3.562575680498601e-05\n",
            "  • aug_prob: 0.2469149578417666\n",
            "\n",
            "📋 WEEK-BY-WEEK COMPARISON:\n",
            "Week   Actual     Forecast   Error      % Error   \n",
            "--------------------------------------------------\n",
            "1      21.88      16.49      -5.39      -24.6     %\n",
            "2      21.02      15.42      -5.60      -26.7     %\n",
            "3      20.57      14.98      -5.59      -27.2     %\n",
            "4      19.11      15.56      -3.55      -18.6     %\n",
            "5      19.53      16.25      -3.28      -16.8     %\n",
            "6      18.08      16.78      -1.30      -7.2      %\n",
            "7      18.73      17.48      -1.25      -6.7      %\n",
            "8      17.68      18.19      0.51       2.9       %\n",
            "9      15.97      18.72      2.75       17.2      %\n",
            "10     15.98      19.11      3.13       19.6      %\n",
            "11     15.09      19.10      4.00       26.5      %\n",
            "12     13.74      20.18      6.44       46.9      %\n",
            "\n",
            "======================================================================\n",
            "\n",
            "💾 Best hyperparameters saved to 'best_hyperparameters.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# CRITICAL FIX: Patch torch.load ONCE at module level\n",
        "# ---------------------------------------------------------------------------\n",
        "import torch.serialization\n",
        "\n",
        "if not hasattr(torch.serialization, '_original_torch_load_backup'):\n",
        "    torch.serialization._original_torch_load_backup = torch.serialization.load\n",
        "\n",
        "def safe_torch_load(*args, **kwargs):\n",
        "    \"\"\"Wrapper that adds weights_only=False by default\"\"\"\n",
        "    if 'weights_only' not in kwargs:\n",
        "        kwargs['weights_only'] = False\n",
        "    return torch.serialization._original_torch_load_backup(*args, **kwargs)\n",
        "\n",
        "# Apply the patch\n",
        "torch.load = safe_torch_load\n",
        "torch.serialization.load = safe_torch_load\n",
        "\n",
        "print(\"✓ torch.load patched successfully\")\n",
        "\n",
        "# GluonTS imports\n",
        "from gluonts.dataset.pandas import PandasDataset\n",
        "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
        "from lag_llama.gluon.estimator import LagLlamaEstimator\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 1. LOAD AND PREPROCESS DATA\n",
        "# ---------------------------------------------------------------------------\n",
        "filename = '/content/macro_index_Financial_Empowerment_Counselling_only.csv'\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    filename = 'macro_index_Financial_Empowerment_Counselling_only.csv'\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    print(f\"❌ Error: {filename} not found. Please upload your CSV file.\")\n",
        "    raise FileNotFoundError(\"Please upload macro_index_Financial_Empowerment_Counselling_only.csv\")\n",
        "\n",
        "df = pd.read_csv(filename)\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.set_index('date', inplace=True)\n",
        "df = df.asfreq('W-SUN')\n",
        "df = df.ffill().bfill()\n",
        "\n",
        "# CRITICAL: Convert all numeric data to float32\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[num_cols] = df[num_cols].astype('float32')\n",
        "\n",
        "print(f\"✓ Data Loaded. Shape: {df.shape}\")\n",
        "print(f\"✓ Date range: {df.index[0]} to {df.index[-1]}\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 2. FEATURE SELECTION\n",
        "# ---------------------------------------------------------------------------\n",
        "target_col = 'Financial Empowerment Counselling'\n",
        "\n",
        "# OPTION: Use NO exogenous features (univariate forecasting)\n",
        "use_exogenous = True\n",
        "\n",
        "if use_exogenous:\n",
        "    selected_features = [\n",
        "        'CPIAUCSL', 'FXEURCAD', 'FXUSDCAD', 'DGS10', 'goc_long_benchmark', 'woodgreen_seniors_care_text_information',\n",
        "        'goc_long_benchmark1', 'goc_avg_over10y', 'DTWEXBGS', 'DFF', 'woodgreen_employment_text_information'\n",
        "    ]\n",
        "    selected_features = [f for f in selected_features if f in df.columns]\n",
        "    print(f\"\\n✓ Using {len(selected_features)} exogenous features\")\n",
        "else:\n",
        "    selected_features = []\n",
        "    print(f\"\\n✓ Using UNIVARIATE forecasting (no exogenous features)\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3. PROPER TRAIN/TEST SPLIT\n",
        "# ---------------------------------------------------------------------------\n",
        "prediction_length = 12\n",
        "context_length = 16\n",
        "\n",
        "# Split data properly - reserve last 12 weeks for testing\n",
        "split_point = len(df) - prediction_length\n",
        "\n",
        "train_df = df.iloc[:split_point]\n",
        "test_df = df.iloc[:split_point]  # TEST DATASET SHOULD NOT INCLUDE FUTURE DATA\n",
        "\n",
        "print(f\"\\n✓ Train dataset: {len(train_df)} weeks (up to {train_df.index[-1]})\")\n",
        "print(f\"✓ Holdout period: {prediction_length} weeks (from {df.index[split_point]})\")\n",
        "print(f\"✓ Last actual value in training: {train_df[target_col].iloc[-1]:.2f}\")\n",
        "\n",
        "# Create datasets\n",
        "if selected_features:\n",
        "    train_ds = PandasDataset(train_df, target=target_col, feat_dynamic_real=selected_features, freq=\"W-SUN\")\n",
        "    test_ds = PandasDataset(test_df, target=target_col, feat_dynamic_real=selected_features, freq=\"W-SUN\")\n",
        "else:\n",
        "    train_ds = PandasDataset(train_df, target=target_col, freq=\"W-SUN\")\n",
        "    test_ds = PandasDataset(test_df, target=target_col, freq=\"W-SUN\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4. LOAD AND FINE-TUNE MODEL\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\nDownloading Lag-Llama checkpoint...\")\n",
        "!huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir .\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"✓ Using device: {device}\")\n",
        "\n",
        "ckpt_path = \"lag-llama.ckpt\"\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
        "\n",
        "estimator = LagLlamaEstimator(\n",
        "    ckpt_path=ckpt_path,\n",
        "    prediction_length=prediction_length,\n",
        "    context_length=context_length,\n",
        "    n_layer=estimator_args[\"n_layer\"],\n",
        "    n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
        "    n_head=estimator_args[\"n_head\"],\n",
        "    scaling=estimator_args[\"scaling\"],\n",
        "    time_feat=estimator_args[\"time_feat\"],\n",
        "    aug_prob=0.2469149578417666,  # Disable augmentations\n",
        "    lr=3.562575680498601e-05,\n",
        "    batch_size=16,  # Reduced for stability\n",
        "    num_parallel_samples=100,\n",
        "    trainer_kwargs={\n",
        "        \"accelerator\": device,\n",
        "        \"max_epochs\": 100,  # Reduced to prevent overfitting\n",
        "    },\n",
        ")\n",
        "\n",
        "print(\"\\n🚀 Starting Fine-Tuning...\")\n",
        "predictor = estimator.train(\n",
        "    training_data=train_ds,\n",
        "    cache_data=True,\n",
        "    shuffle_buffer_length=1000\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 5. PREDICT AND EVALUATE\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n📊 Generating Forecasts...\")\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_ds,\n",
        "    predictor=predictor,\n",
        "    num_samples=100\n",
        ")\n",
        "\n",
        "forecasts = list(forecast_it)\n",
        "tss = list(ts_it)\n",
        "\n",
        "# Get actual values for the forecast period\n",
        "actual_values = df[target_col].iloc[split_point:split_point + prediction_length].values\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6. CALCULATE METRICS\n",
        "# ---------------------------------------------------------------------------\n",
        "ts_entry = tss[0]\n",
        "forecast_entry = forecasts[0]\n",
        "forecast_mean = forecast_entry.mean\n",
        "\n",
        "# Calculate errors\n",
        "mae = np.mean(np.abs(forecast_mean - actual_values))\n",
        "rmse = np.sqrt(np.mean((forecast_mean - actual_values) ** 2))\n",
        "mape = np.mean(np.abs((actual_values - forecast_mean) / actual_values)) * 100\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 7. PLOT RESULTS\n",
        "# ---------------------------------------------------------------------------\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "# ========== PLOT 1: FORECAST WITH HISTORICAL CONTEXT ==========\n",
        "ts_index = ts_entry[-100:].index.to_timestamp()\n",
        "ts_values = ts_entry[-100:].values\n",
        "\n",
        "ax1.plot(ts_index, ts_values, label=\"Historical Data\", linewidth=2,\n",
        "         color='#2E86AB', marker='o', markersize=3, alpha=0.8)\n",
        "\n",
        "forecast_index = forecast_entry.index.to_timestamp()\n",
        "\n",
        "# Plot actual values in forecast period\n",
        "actual_index = df.index[split_point:split_point + prediction_length]\n",
        "ax1.plot(actual_index, actual_values, label=\"Actual (Holdout)\",\n",
        "         linewidth=2.5, color='#E63946', marker='o', markersize=5)\n",
        "\n",
        "# Plot forecast\n",
        "ax1.plot(forecast_index, forecast_mean, label=\"Forecast Mean\",\n",
        "         linewidth=2.5, color='#06A77D', marker='s', markersize=4, linestyle='--')\n",
        "\n",
        "# Prediction intervals\n",
        "q05 = forecast_entry.quantile('0.05')\n",
        "q95 = forecast_entry.quantile('0.95')\n",
        "ax1.fill_between(forecast_index, q05, q95, alpha=0.2, color='#06A77D',\n",
        "                 label='90% Prediction Interval')\n",
        "\n",
        "ax1.axvline(x=forecast_index[0], color='red', linestyle='--',\n",
        "           linewidth=1.5, alpha=0.7, label='Forecast Start')\n",
        "\n",
        "ax1.set_title(f\"Weekly Intake Forecast vs Actual\\n\"\n",
        "             f\"MAE: {mae:.2f} | RMSE: {rmse:.2f} | MAPE: {mape:.1f}%\",\n",
        "             fontsize=14, fontweight='bold', pad=15)\n",
        "ax1.set_xlabel(\"Date\", fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel(\"Number of People\", fontsize=12, fontweight='bold')\n",
        "ax1.legend(fontsize=9, loc='best', framealpha=0.9)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
        "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# ========== PLOT 2: FORECAST ERROR ANALYSIS ==========\n",
        "errors = forecast_mean - actual_values\n",
        "weeks = np.arange(1, prediction_length + 1)\n",
        "\n",
        "ax2.bar(weeks, errors, color=['#06A77D' if e >= 0 else '#E63946' for e in errors],\n",
        "       alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "ax2.set_title(\"Forecast Error by Week\", fontsize=14, fontweight='bold', pad=15)\n",
        "ax2.set_xlabel(\"Week Ahead\", fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel(\"Error (Forecast - Actual)\", fontsize=12, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
        "ax2.set_xticks(weeks)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 8. PRINT DETAILED SUMMARY\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ FORECAST EVALUATION COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n📊 ACCURACY METRICS:\")\n",
        "print(f\"  • MAE (Mean Absolute Error):        {mae:.2f}\")\n",
        "print(f\"  • RMSE (Root Mean Squared Error):   {rmse:.2f}\")\n",
        "print(f\"  • MAPE (Mean Absolute % Error):     {mape:.1f}%\")\n",
        "\n",
        "print(f\"\\n📈 FORECAST SUMMARY (Next {prediction_length} weeks):\")\n",
        "print(f\"  • Mean Forecast:     {forecast_mean.mean():.2f}\")\n",
        "print(f\"  • Median Forecast:   {np.median(forecast_mean):.2f}\")\n",
        "print(f\"  • Min Forecast:      {forecast_mean.min():.2f}\")\n",
        "print(f\"  • Max Forecast:      {forecast_mean.max():.2f}\")\n",
        "print(f\"  • Std Deviation:     {forecast_mean.std():.2f}\")\n",
        "\n",
        "print(f\"\\n📉 ACTUAL VALUES (Holdout Period):\")\n",
        "print(f\"  • Mean Actual:       {actual_values.mean():.2f}\")\n",
        "print(f\"  • Min Actual:        {actual_values.min():.2f}\")\n",
        "print(f\"  • Max Actual:        {actual_values.max():.2f}\")\n",
        "\n",
        "print(f\"\\n📋 WEEK-BY-WEEK COMPARISON:\")\n",
        "print(f\"{'Week':<6} {'Actual':<10} {'Forecast':<10} {'Error':<10} {'% Error':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for i, (actual, pred) in enumerate(zip(actual_values, forecast_mean), 1):\n",
        "    error = pred - actual\n",
        "    pct_error = (error / actual) * 100\n",
        "    print(f\"{i:<6} {actual:<10.2f} {pred:<10.2f} {error:<10.2f} {pct_error:<10.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {
        "id": "l8R9IPHnl_zo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ea85154378304b76a3f01218b9d67fdb",
            "672e4acfa83d4e5aae0862545121926e",
            "eaad0efda1f6492895889d662b8fbf48",
            "446541ee2dc24daaa5406a7eb4453e83",
            "93f4a96f3c2e4aaf8352b67d09c53ed8",
            "d5c09c5cb7c442e792e4eb5630595978",
            "900781e8f0ed4e64b5b91f2fb55ac9ef",
            "ed2adbee5ad94a0790d8615b7eb4b53a",
            "d01ba1409095411696e7faf95abeffc9",
            "48570123bfb7483db8c12f5036c7d06a",
            "5b50a03e7f5249e992cde7c0818851a4"
          ]
        },
        "outputId": "36a8e53e-d0bb-42a5-e0e9-b56ad1b1bd14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ torch.load patched successfully\n",
            "✓ Data Loaded. Shape: (242, 44)\n",
            "✓ Date range: 2021-03-07 00:00:00 to 2025-10-19 00:00:00\n",
            "\n",
            "✓ Using 11 exogenous features\n",
            "\n",
            "✓ Train dataset: 230 weeks (up to 2025-07-27 00:00:00)\n",
            "✓ Holdout period: 12 weeks (from 2025-08-03 00:00:00)\n",
            "✓ Last actual value in training: 13.00\n",
            "\n",
            "Downloading Lag-Llama checkpoint...\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "lag-llama.ckpt\n",
            "✓ Using device: cuda\n",
            "\n",
            "🚀 Starting Fine-Tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type               | Params\n",
            "-----------------------------------------------------\n",
            "0 | model         | LagLlamaModel      | 2.4 M \n",
            "1 | augmentations | ApplyAugmentations | 0     \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.797     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea85154378304b76a3f01218b9d67fdb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n",
            "INFO: Epoch 0, global step 50: 'train_loss' reached 1.49040 (best 1.49040), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 50: 'train_loss' reached 1.49040 (best 1.49040), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
            "INFO: Epoch 1, global step 100: 'train_loss' reached 0.80675 (best 0.80675), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 100: 'train_loss' reached 0.80675 (best 0.80675), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
            "INFO: Epoch 2, global step 150: 'train_loss' reached 0.53893 (best 0.53893), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 150: 'train_loss' reached 0.53893 (best 0.53893), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
            "INFO: Epoch 3, global step 200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 4, global step 250: 'train_loss' reached 0.35144 (best 0.35144), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 250: 'train_loss' reached 0.35144 (best 0.35144), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
            "INFO: Epoch 5, global step 300: 'train_loss' reached -0.21761 (best -0.21761), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 300: 'train_loss' reached -0.21761 (best -0.21761), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
            "INFO: Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 11, global step 600: 'train_loss' reached -0.53521 (best -0.53521), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 600: 'train_loss' reached -0.53521 (best -0.53521), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
            "INFO: Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 15, global step 800: 'train_loss' reached -0.63196 (best -0.63196), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 800: 'train_loss' reached -0.63196 (best -0.63196), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
            "INFO: Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 21, global step 1100: 'train_loss' reached -0.65568 (best -0.65568), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 1100: 'train_loss' reached -0.65568 (best -0.65568), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
            "INFO: Epoch 22, global step 1150: 'train_loss' reached -0.67776 (best -0.67776), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 1150: 'train_loss' reached -0.67776 (best -0.67776), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
            "INFO: Epoch 23, global step 1200: 'train_loss' reached -0.76614 (best -0.76614), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 1200: 'train_loss' reached -0.76614 (best -0.76614), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
            "INFO: Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 1250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 1300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 27, global step 1400: 'train_loss' reached -1.21069 (best -1.21069), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 1400: 'train_loss' reached -1.21069 (best -1.21069), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
            "INFO: Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 30, global step 1550: 'train_loss' reached -1.37629 (best -1.37629), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 1550: 'train_loss' reached -1.37629 (best -1.37629), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=30-step=1550.ckpt' as top 1\n",
            "INFO: Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 1700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 34, global step 1750: 'train_loss' reached -1.43351 (best -1.43351), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 1750: 'train_loss' reached -1.43351 (best -1.43351), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
            "INFO: Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 43, global step 2200: 'train_loss' reached -1.52478 (best -1.52478), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=43-step=2200.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 2200: 'train_loss' reached -1.52478 (best -1.52478), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=43-step=2200.ckpt' as top 1\n",
            "INFO: Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 50, global step 2550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 51, global step 2600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 52, global step 2650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 53, global step 2700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 54, global step 2750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 55, global step 2800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 56, global step 2850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 57, global step 2900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 58, global step 2950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 59, global step 3000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 60, global step 3050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 61, global step 3100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 62, global step 3150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 63, global step 3200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 64, global step 3250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 65, global step 3300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 66, global step 3350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 67, global step 3400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 68, global step 3450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 69, global step 3500: 'train_loss' reached -1.68347 (best -1.68347), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=69-step=3500.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 69, global step 3500: 'train_loss' reached -1.68347 (best -1.68347), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=69-step=3500.ckpt' as top 1\n",
            "INFO: Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 70, global step 3550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 71, global step 3600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 72, global step 3650: 'train_loss' reached -1.96503 (best -1.96503), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=72-step=3650.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 72, global step 3650: 'train_loss' reached -1.96503 (best -1.96503), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=72-step=3650.ckpt' as top 1\n",
            "INFO: Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 73, global step 3700: 'train_loss' was not in top 1\n",
            "INFO: Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 74, global step 3750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 75, global step 3800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 76, global step 3850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 77, global step 3900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 78, global step 3950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 79, global step 4000: 'train_loss' was not in top 1\n",
            "INFO: Epoch 80, global step 4050: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 80, global step 4050: 'train_loss' was not in top 1\n",
            "INFO: Epoch 81, global step 4100: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 81, global step 4100: 'train_loss' was not in top 1\n",
            "INFO: Epoch 82, global step 4150: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 82, global step 4150: 'train_loss' was not in top 1\n",
            "INFO: Epoch 83, global step 4200: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 83, global step 4200: 'train_loss' was not in top 1\n",
            "INFO: Epoch 84, global step 4250: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 84, global step 4250: 'train_loss' was not in top 1\n",
            "INFO: Epoch 85, global step 4300: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 85, global step 4300: 'train_loss' was not in top 1\n",
            "INFO: Epoch 86, global step 4350: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 86, global step 4350: 'train_loss' was not in top 1\n",
            "INFO: Epoch 87, global step 4400: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 87, global step 4400: 'train_loss' was not in top 1\n",
            "INFO: Epoch 88, global step 4450: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 88, global step 4450: 'train_loss' was not in top 1\n",
            "INFO: Epoch 89, global step 4500: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 89, global step 4500: 'train_loss' was not in top 1\n",
            "INFO: Epoch 90, global step 4550: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 90, global step 4550: 'train_loss' was not in top 1\n",
            "INFO: Epoch 91, global step 4600: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 91, global step 4600: 'train_loss' was not in top 1\n",
            "INFO: Epoch 92, global step 4650: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 92, global step 4650: 'train_loss' was not in top 1\n",
            "INFO: Epoch 93, global step 4700: 'train_loss' reached -2.04271 (best -2.04271), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=93-step=4700.ckpt' as top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 93, global step 4700: 'train_loss' reached -2.04271 (best -2.04271), saving model to '/content/lag-llama/lightning_logs/version_53/checkpoints/epoch=93-step=4700.ckpt' as top 1\n",
            "INFO: Epoch 94, global step 4750: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 94, global step 4750: 'train_loss' was not in top 1\n",
            "INFO: Epoch 95, global step 4800: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 95, global step 4800: 'train_loss' was not in top 1\n",
            "INFO: Epoch 96, global step 4850: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 96, global step 4850: 'train_loss' was not in top 1\n",
            "INFO: Epoch 97, global step 4900: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 97, global step 4900: 'train_loss' was not in top 1\n",
            "INFO: Epoch 98, global step 4950: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 98, global step 4950: 'train_loss' was not in top 1\n",
            "INFO: Epoch 99, global step 5000: 'train_loss' was not in top 1\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Epoch 99, global step 5000: 'train_loss' was not in top 1\n",
            "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "/usr/local/lib/python3.12/dist-packages/gluonts/torch/util.py:205: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  return a[idx]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Generating Forecasts...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "✅ FORECAST EVALUATION COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "📊 ACCURACY METRICS:\n",
            "  • MAE (Mean Absolute Error):        8.79\n",
            "  • RMSE (Root Mean Squared Error):   10.29\n",
            "  • MAPE (Mean Absolute % Error):     66.6%\n",
            "\n",
            "📈 FORECAST SUMMARY (Next 12 weeks):\n",
            "  • Mean Forecast:     19.68\n",
            "  • Median Forecast:   19.17\n",
            "  • Min Forecast:      10.83\n",
            "  • Max Forecast:      29.17\n",
            "  • Std Deviation:     5.86\n",
            "\n",
            "📉 ACTUAL VALUES (Holdout Period):\n",
            "  • Mean Actual:       16.08\n",
            "  • Min Actual:        9.00\n",
            "  • Max Actual:        26.00\n",
            "\n",
            "📋 WEEK-BY-WEEK COMPARISON:\n",
            "Week   Actual     Forecast   Error      % Error   \n",
            "--------------------------------------------------\n",
            "1      26.00      13.77      -12.23     -47.0     %\n",
            "2      18.00      10.83      -7.17      -39.9     %\n",
            "3      19.00      11.86      -7.14      -37.6     %\n",
            "4      14.00      19.15      5.15       36.8      %\n",
            "5      21.00      17.00      -4.00      -19.0     %\n",
            "6      13.00      19.18      6.18       47.5      %\n",
            "7      21.00      20.39      -0.61      -2.9      %\n",
            "8      14.00      19.17      5.17       37.0      %\n",
            "9      10.00      19.79      9.79       97.9      %\n",
            "10     16.00      26.82      10.82      67.6      %\n",
            "11     12.00      29.17      17.17      143.1     %\n",
            "12     9.00       29.06      20.06      222.9     %\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}